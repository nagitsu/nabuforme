\chapter{Evaluación}

Contando ya con la herramienta y el corpus de texto en español, se procede a construir
representaciones vectoriales empleando los principales algoritmos en el área y a evaluar su
comportamiento en el idioma español. Con el fin de tener valores de referencia con los cuales
comparar los modelos entrenados, se traducen del inglés al español los conjuntos de pruebas más
utilizados en la literatura.

El capítulo comienza detallando el proceso de entrenamiento, enumerando cuáles son los modelos
vectoriales entrenados y qué hiperparámetros estos utilizan. Luego se expone la comparación de
dichos modelos con los resultados obtenidos para el inglés, presentando cuáles son los principales
conjuntos de pruebas empleados. También se realiza un estudio sobre la performance de los modelos
frente a pruebas diseñadas para evaluar particularidades del idioma español. Por último, se realiza
un análisis cualitativo de las representaciones, mostrando algunos ejemplos de su comportamiento de
manera más informal pero ilustrativa.


\section{Modelos Entrenados}

Como se mencionó anteriormente, se optó por entrenar representaciones vectoriales siguiendo los tres
principales enfoques de la literatura: modelos neuronales a través del algoritmo \texttt{word2vec},
modelos estadísticos mediante el empleo de una matriz PPMI con SVD, y modelos híbridos mediante el
algoritmo \texttt{GloVe}. El primero y el último utilizaron implementaciones ya existentes
(\texttt{Gensim} y la provista por los autores, respectivamente), mientras que el segundo fue una
implementación propia siguiendo los lineamientos de [Levy y Goldberg cita].

Cada uno de estos modelos tiene distintos hiperparámetros que pueden afectar significativamente su
performance. En [Levy y Goldberg] los autores buscan unificar los hiperparámetros de estos modelos,
proponiendo un subconjunto de los mismos que contempla las tres variantes. El enfoque tomado en este
proyecto no llega a tal profundidad, pues quita parte de la versatilidad de algunos algoritmos, pero
intenta aún así presentarlos de manera unificada cuando es posible.

En el caso de \texttt{word2vec}, los hiperparámetros disponibles para modificar son:

\begin{itemize}

\item \textbf{Dimensión} (\texttt{dim}): dimensión de las representaciones inducidas (valor por
defecto: 300).

\item \textbf{Cantidad de ocurrencias mínima} (\texttt{min\_count}): cantidad mínima de ocurrencias
que una palabra debe tener para ser considerada parte del vocabulario (valor por defecto: 5). En
caso de no llegar a dicho límite, se excluye del corpus como si no estuviera presente.

\item \textbf{Ventana de contexto} (\texttt{win}): tamaño de la ventana de contexto utilizada hacia
cada lado de la palabra central (valor por defecto: 5). Notar que no se estudian variantes con
contextos asimétricos o con otras características.

\item \textbf{Subsampling} (\texttt{sub}): qué nivel de \textit{subsampling} realizar a las palabras
muy frecuentes, para evitar que sean sobrerrepresentadas en los vectores finales (valor por defecto:
0.00001). Hacer subsampling de una palabra implica removerla de una oración con una probabilidad
$p$\footnote{Se realiza subsampling si $\mathtt{sub} \times |V| > \#w$, donde $|V|$ es el tamaño del
vocabulario y $\#w$ la cantidad de ocurrencias de la palabra $w \in V$. En ese caso, el subsampling
se realiza con una probabilidad $p = 1 - \sqrt{\frac{\mathtt{sub}}{\#w}}$, siguiendo la sugerencia
de [Levy y Goldberg cita].}.

\item \textbf{Algoritmo empleado} (\texttt{algo}): qué algoritmo de \texttt{word2vec} utilizar,
Skipgram (\texttt{sg}) o CBOW (\texttt{cbow}) (valor por defecto: \texttt{sg}).\ Como fue mencionado
en el capítulo 2, el primero busca predecir la palabra central en base a su contexto y el segundo lo
opuesto.

\item \textbf{Variante utilizada} (\texttt{var}): si construir y utilizar un softmax jerárquico para
la estimación de la probabilidad (\texttt{hs}) o si emplear la técnica de \textit{negative sampling}
(\texttt{ns}) (valor por defecto: \texttt{ns}).

\item \textbf{Cantidad de muestras negativas} (\texttt{neg}): en caso de utilizar negative sampling,
la cantidad de muestras negativas a tomar por cada ejemplo positivo (valor por defecto: 7).

\item \textbf{Ritmo de aprendizaje} (\texttt{alpha}): ritmo de aprendizaje a utilizar en el empleo
del descenso estocástico por el gradiente (SGD) mediante el cuál se aproximan las probabilidades
(valor por defecto: $0.025$). Un mayor valor para este parámetro corre el riesgo de que SGD no
converja, mientras que un menor valor puede hacer que se requieran de múltiples recorridas sobre el
corpus para obtener resultados aceptables. De todos modos, este valor es el ritmo inicial, e irá
disminuyendo conforme avance el entrenamiento para ayudar a la convergencia.

\item \textbf{Recorridas sobre el corpus} (\texttt{epoch}): cantidad de veces que se recorre el
corpus de texto (valor por defecto: 1). Recorrer un mayor número de veces el corpus de entrenamiento
mejora la convergencia del algoritmo pero, obviamente, hará que aumente el tiempo de entrenamiento
drásticamente.

\end{itemize}

El modelo \texttt{GloVe} presenta mucho de los mismos hiperparámetros (en particular, \texttt{dim},
\texttt{min\_count}, \texttt{win}, \texttt{alpha}, y \texttt{epoch}), con algunos hiperparámetros
adicionales:

\begin{itemize}

\item \textbf{Tamaño máximo del vocabulario} (\texttt{max\_count}): cantidad máxima de palabras que
puede contener el vocabulario donde, en caso de alcanzarse, se utilizan únicamente las
\texttt{max\_count} palabras más frecuentes (valor por defecto: \textit{sin máximo}).

\item \textbf{Cota de la función de peso} (\texttt{x\_max}): cota superior a utilizar para la
métrica de asociación entre los pares de las palabras en la matriz de coocurrencia intermedia (valor
por defecto: $100$). Si la métrica asume valores superiores a $100$, se toma el valor $100$ en su
lugar.

\item \textbf{Exponente para la función de peso} (\texttt{eta}): exponente a utilizar en la función
de peso, tal como es definido en [glove cita] (valor por defecto: $0.75$). Tanto este parámetro como
el anterior fueron dejados en sus valores por defecto en todos los modelos generados, puesto que son
muy particulares al algoritmo y su estudio escapa el alcance del proyecto. De todos modos, como
comentan los propios autores, su valor no debería ser determinante en la performance de las
representaciones resultantes.

\end{itemize}

Cabe notar, sin embargo, que el hiperparámetro \texttt{epoch} tiene una importancia mayor en este
algoritmo, pues no implica una recorrida por todo el corpus sino por la matriz de coocurrencias
intermedia que se genera (en particular, por el vocabulario empleado). \texttt{epoch} en este
contexto indica la cantidad de iteraciones empleada en la generación de la matriz reducida, y debe
ser un número alto para asegurar una correcta factorización. El valor por defecto es 15, aunque los
autores sugieren utilizar entre 50 y 100 iteraciones.

Por último, el modelo PPMI con SVD tiene también algunos hiperparámetros adicionales (\texttt{dim},
\texttt{min\_count}, \texttt{max\_count}, \texttt{win}, \texttt{sub}) con la incorporación de dos
adicionales:

\begin{itemize}

\item \textbf{Suavizado de la distribución del contexto} (\texttt{cds}): suavizado empleado para las
coocurrencias de las palabras en la construcción de la matriz de coocurrencia, tal como es detallado
en [levy goldberg cita] (valor por defecto: $0.75$). Este parámetro disminuye el sesgo natural que
la medida PMI tiene con las palabras raras, e incrementa consistentemente la performance sobre la
opción sin suavizado (valor $1.0$). Por esta razón, en los modelos entrenados siempre se utilizó el
valor por defecto.

\item \textbf{Suma de vectores de contexto} (\texttt{w+c}): luego de aplicar SVD a la matriz de
coocurrencias, se obtiene una matriz que asocia las palabras del vocabulario al espacio reducido y
otra que asocia las palabras de los contextos al mismo espacio (valor por defecto:
\texttt{true}). En [Levy Goldberg cita], los autores proponen utilizar la suma de ambos vectores en
lugar de únicamente los vectores base, obteniendo en algunos casos mejores resultados.

\end{itemize}

Este último modelo tal como es planteado por los autores cuenta con algunos hiperparámetros
adicionales, como la normalización de los vectores resultantes y la variante de~\cite{Caron2001} en
SVD\@. Sin embargo, puesto que los resultados a los que llegan los autores sistemáticamente
favorecen a una variante particular, se optó por dejarlos fijos.


Por otro lado, también se cuenta con hiperparámetros para el preprocesamiento del corpus de texto
previo a ser alimentado a los algoritmos. En particular, se tienen dos opciones: \textbf{remover
tildes} (\texttt{acc}), que remueve todo tipo de tildes del texto previo al entrenamiento, y
\textbf{pasar a minúsculas} (\texttt{lower}), que pasa todo el texto a minúsculas, no haciendo
distinción entre \textit{Papa} y \textit{papa}.

También está la posibilidad de evaluar distintos mecanismos para la tokenización de oraciones y
palabras, pero se decidió limitar el estudio a los dos anteriores. En en el caso de la tokenización
de palabras, se partió las oraciones en secuencias de caracteres alfanuméricos, descartando todo
tipo de símbolo distinto.


La gran cantidad de hiperparámetros disponibles para cada algoritmo deja un espacio de búsqueda
extremadamente grande, que no es posible explorar por completo. Por este motivo se construyó sobre
el trabajo previo en la literatura, utilizando los valores de parámetros que mejores resultados han
obtenido y explorando en la proximidad de cada uno.

Procediendo de este modo, es posible obtener una mejor visión de cómo funcionan los distintos
algoritmos en su versión equivalente en español, replicando los comportamientos obtenidos en
inglés. Además, probando alternativas de los hiperparámetros no necesariamente estudiadas en la
literatura, se evita que se escapen variantes particulares o prometedoras.

Los valores por defecto de cada hiperparámetro son los que mejores resultados han obtenido en la
literatura, buscando que si se deja sin fijar uno de ellos, el modelo resultante sea aún así
razonablemente bueno. En el caso de \texttt{word2vec}, el modelo arquetípico utiliza la variante
Skipgram con negative sampling (utilizando 7 muestras negativas); establece una dimensión resultante
de 300, y una ventana de contexto de ancho 5; un subsampling de $1\times10^{-5}$, 5 ocurrencias
mínimas de una palabra para entrar al vocabulario y una única recorrida del corpus.

Para \texttt{GloVe} se parte de valores similares: 300 componentes en los vectores resultantes, al
menos 5 ocurrencias para ingresar al vocabulario, y una ventana de tamaño 5. No se limita el tamaño
máximo del vocabulario y, como se mencionó anteriormente, se utilizan los valores propuestos por los
autores del algoritmo para la cota y el exponente de la función de peso. Por último, se realizan 15
recorridas por la matriz de coocurrencias.

El modelo estadístico se basa también en los valores de hiperparámetros de los otros dos
algoritmos. Para el suavizado de la distribución de contexto se utiliza, como se explicó antes, un
valor de $0.75$ (y no se varió en ningún modelo), mientras que se realiza la suma de vectores de
contexto.

Algunos de estos hiperparámetros, como la dimensión de los vectores resultantes o el tamaño de la
ventana, dependen del tamaño del corpus que se utilice para entrenarlos, donde a mayores valores
funcionarán bien con más datos. De todos modos, los valores elegidos para éstos son razonables para
los volúmenes de texto que se manejan.

Por otro lado, hay hiperparámetros cuyos valores dependen de otros, como la cantidad de muestras
negativas en \texttt{word2vec} (que no aplica cuando se utiliza un softmax jerárquico). Otro ejemplo
son las limitantes al tamaño del vocabulario, donde \texttt{max\_count}, \texttt{min\_count}, e
incluso la dimensión y el tamaño de la ventana de contexto son utilizadas en conjunto para acotar
los requerimientos computacionales de correr los algoritmos.

Relacionado a este último punto, la cantidad de recorridas sobre el corpus en \texttt{word2vec} o de
la matriz de coocurrencias en \texttt{GloVe} tienen una influencia directa en el tiempo de cómputo
del que se disponga. Para este tipo de hiperparámetros, se tomaron valores que permitan entrenar
adecuadamente los modelos en el servidor del que se disponía, buscando mantener el tiempo de
entrenamiento para la mayoría de los vectores por debajo de un día.

Sobre estos valores base se entrenan a su vez alternativas, comparando los resultados al variar
hiperparámetros particulares, donde se mantiene el resto de los valores fijos. De esta forma se
busca estudiar la influencia que tiene, por ejemplo, la dimensión de los vectores o la cantidad de
recorridas por el corpus en la performance de las representaciones vectoriales.


En cuanto al corpus de entrenamiento empleado, se entrenaron dos grandes grupos de modelos
vectoriales: uno con un corpus de $4000$ millones de palabras de todos las fuentes (con la excepción
del texto de baja calidad obtenido de los foros) y otro de $400$ millones de palabras entrenado
exclusivamente con noticias de Uruguay, ambos con los tildes removidos. Se toman estas dos variatnes
con el objetivo de estudiar cómo afecta el tamaño del corpus en la calidad de los vectores
resultantes. Estos grupos de modelos entrenados con texto genérico, a su vez, buscan ser comparables
con las representaciones que se entrenan en la literatura, los cuales utilizan corpus de texto que
tratan de no influir a los vectores de ninguna manera particular.

Además de estos dos conjuntos se entrenaron modelos adicionales con datos exclusivamente de España
(con alrededor de $400$ millones de palabras), con datos provenientes tanto de noticias como de
foros, con los datos anteriores pero sin los tildes removidos, y con la totalidad del corpus
obtenido finalmente ($6000$ millones de palabras). Estas variantes se pensaron con el fin de
estudiar cómo difiere el comportamiento de los vectores según el texto con el que se alimenta.

En la tabla [tabla] se listan todas las variantes de corpus entrenados, junto con un código para
identificarlos fácilmente, mientras que en la tabla [tabla] se listan todos los modelos vectoriales,
especificando los hiperparámetros empleados para cada uno y el corpus utilizado.

[tabla corpus: id, descripción específica (e.g. docs scrapeados antes de... (footnote con
explicación de por qué cualquier cosa)), cantidad palabras, preprocessing options]

[tabla embeddings: id, descripción?, corpus utilizado, tiempo entr.; hiperparámetros; seccionar por
embedding type así puedo poner columna por hiperparámetro?]


El entrenamiento de los modelos vectoriales fue realizado a través de la herramienta construida y
detallada en el capítulo 4. Los hiperparámetros a evaluar se fijaron a priori, dando de alta los
\textit{embeddings} en la interfaz web y mandando posteriormente a entrenar cada uno de ellos.

Como se puede apreciar en la tabla [tabla embeddings], se construyeron [num] representaciones
vectoriales distintas: [num] utilizando \texttt{word2vec}, [num] utilizando \texttt{GloVe}, y [num]
utilizando PPMI\@. El tiempo de entrenamiento promedio de cada modelo fue de [time2], donde el más
rápido tomó [time0] ([cualfue]) y el más lento [time1] ([cualfue]). Existe una diferencia
significativa entre el tiempo empleado en ambos extremos, lo que resalta la importancia de la
limitante que impone la capacidad de cómputo disponible al entrenamiento de vectores. En total, se
invirtió [time3] entrenando modelos vectoriales.

Cada modelo vectorial ocupa entre 300MB y 1000MB (con algunos llegando a 1700MB), dependiendo
principalmente de la dimensión elegida y los límites que se le establezcan al vocabulario (cantidad
mínima de ocurrencias, o directamente el tamaño máximo del vocabulario). Esto no genera grandes
presiones en el almacenamiento, aunque aún así no es un tema menor a la hora de descargar los
modelos o de cargarlos en memoria, pues tiene un tiempo de arranque no despreciable.


\section{Comparación con la Literatura}

Uno de los principales objetivos del proceso de evaluación es comparar la calidad de las
representaciones vectoriales generadas con texto en español con las generadas con texto en
inglés. Puesto que todos los conjuntos de pruebas son inherentemente dependientes al idioma de los
vectores, no es posible realizar una comparación perfecta, en especial debido a la fuerte
dependencia que tiene la calidad de los vectores con el corpus de entrenamiento empleado.

De todos modos, y para tener al menos un punto de comparación inicial entre ambos idiomas, se optó
por tomar los principales conjuntos de pruebas utilizados en la literatura inglesa y traducirlos al
idioma español, adaptando las pruebas cuando fuera necesario (por ejemplo, cuando se evalúan
cualidades sintácticas no existentes en español). Así, es posible obtener una referencia aproximada
de qué tanto se acercan los modelos entrenados a los obtenidos por la comunidad.

Un trabajo similar se realiza en... [mencionar algún otro paper que ya lo haya hecho]

Se emplean dos tipos de pruebas para realizar la comparación: pruebas basadas en analogías y pruebas
basadas en similitud de palabras. Como ya fue mencionado en capítulos anteriores, la primera busca
recuperar una palabra en base a otras tres palabras dadas, mientras la segunda busca asignar un
puntaje de similitud entre pares de palabras.

Para las analogías existen dos conjuntos de pruebas muy utilizados por la comunidad: el conjunto de
analogías de Microsoft (MSR) [cita], compuesto exclusivamente por analogías que buscan regularidades
sintácticas en los vectores, y el conjunto de Google [cita], compuesto tanto por analogías
sintácticas como semánticas. De estos dos, se optó por utilizar únicamente el de Google, pues el
MSR tiene demasiadas pruebas sintácticas específicas al inglés (como posesivos, comparativos, y
superlativos) que deben ser descartadas en el proceso de traducción, haciendo que el conjunto
resultante sea tan distinto que una comparación de los resultados sería insustancial.

Debido a la antigüedad del campo, existe una gran cantidad de conjuntos de pruebas para la tarea de
similitud de palabras, en especial en lo que respecta a la evaluación de modelos estadísticos. Se
decidió, para limitar la cantidad de conjuntos a un número razonable, elegir las pruebas más
recientemente utilizados por la literatura, con el fin de tener comparaciones más fiables. En
particular, se toman los conjuntos empleados por [Levy y Goldberg], pues son evaluados con los tres
tipos de modelos empleados en el proyecto. Estos conjuntos son:

\begin{itemize}

\item WordSim [cita], un conjunto compuesto por 353 pares de palabras puntuados en base a la
relación entre ambas palabras (si los jueces humanos consideran que las palabras están relacionadas
de alguna manera cualquiera) y su similitud (si existe una relación más fuerte entre los pares, ya
sea sinonimia, hiponimia, etc.). Se utiliza el conjunto partido entre los pares que miden relación y
los que miden similitud, como se hace en [cita].

\item MEN [cita bruni et al], compuesto por 3000 pares de palabras elegidas al azar entre las
palabras que aparecen al menos 700 veces en el corpus WaCky [cita]. Los autores utilizaron la
plataforma Mechanical Turk [cita] para la construcción del conjunto, y el puntaje se asigna en base
a comparaciones binarias entre entradas del conjunto.

\item El conjunto MTurk [cita radinsky et al], compuesto de 287 pares de palabras, con puntajes
también asignados a partir de la herramienta Mechanical Turk.

\item SimLex [cita hill et al], compuesto de 999 pares puntuados exclusivamente en base a la
similitud entre las palabras. Esto implica que pares como \textit{ropa} y \textit{ropero} tienen
asignado un puntaje bajo, por no ser estrictamente similares. Este conjunto es particularmente
complicado de resolver por parte de los modelos vectoriales, pues éstos aprenden en base a la
coocurrencia de pares y la coocurrencia refleja principalmente la relación que existe entre las
palabras.

\end{itemize}

Por otro lado, se descarta el conjunto de palabras raras propuesto por [Luong et al] debido a que la
sutileza entre los pares de palabras empleados se perdería en el proceso de traducción.


Los cinco conjuntos de pruebas seleccionados para realizar la comparación debieron ser traducidos
manualmente, pues no fue posible encontrar traducciones satisfactorias de los mismos.

Se construyó un script que aprovecha el servicio de Google Translate [cita] para realizar una
traducción semi-automática de las pruebas. Este script lee una lista de palabras de entrada, realiza
un pedido HTTP al servicio y obtiene la primer traducción sugerida. Esta palabra se toma
posteriormente como una sugerencia que es revisada manualmente, pero que no obstante ayuda a
agilizar la tarea.

El proceso de traducción para cada conjunto contó, pues, de cuatro etapas: en primer lugar, se
corrió el script para la lista de palabras del conjunto; luego se recorrió la lista de palabras
traducidas, comparándolas con las versiones en inglés y se corrigieron o eliminan las traducciones
incorrectas, ya sea por estar mal la traducción en sí o porque no era el sentido que esperaba en la
prueba; en tercer lugar, se sustituyen las traducciones por las palabras originales en los conjuntos
de prueba, eliminando la entrada si no se encuetra la traducción; por último, se recorre la lista
pruebas buscando posibles errores y eliminando o corrigiendo entradas adicionales.

El proceso de traducción, sin embargo, es falible, por lo que es de esperar que existan traducciones
incorrectas en los conjuntos de pruebas, aunque la proporción no sea alta. Puesto que algunos
conjuntos de pruebas llegan a tener más de diez mil líneas, la revisión final fue más bien
superficial.

En el caso de las tareas de similitud de palabra, donde se encuentra frecuentemente diferencias
finas en el significado de los pares de palabras, se buscó que las traducciones siguieran la
intención de los autores. Por ejemplo, mientras que Google Translate sugería la traducción
\textit{costa} tanto para \textit{shore} como para \textit{coast}, se tomó la decisión de traducir
\textit{shore} como \textit{orilla}, que es una palabra también válida pero que se ajusta al puntaje
que el par tenía asociado. Esto indudablemente está sesgando el conjunto de pruebas, pero es la
única alternativa para realizar una comparación, fuera de construir los conjuntos de nuevo como
hicieron los autores originales.

En el conjunto de analogías de Google fue necesario eliminar por completo algunas secciones por no
ser pertinentes al idioma español. En particular, se eliminaron las categorías
\texttt{gram3-comparative} y \texttt{gram4-superlative}, que tienen analogías entre adjetivos y sus
comparativos o sus superlativos (e.g. \textit{great}, \textit{greater}, y \textit{greatest}), pues
no tienen traducciones al español de una única palabra. También se decidió eliminar la sección
\texttt{city-in-state}, que tiene analogías de ciudades y estados de EE.UU., pues nuestro corpus de
texto se basa en fuentes hispanas y no tendrá muchas menciones de las mismas.

En la tabla [tabla testsets] se resumen los conjuntos de pruebas empleados para la evaluación, junto
con la cantidad de entradas originales y resultantes de cada uno. Como se puede apreciar, las
pruebas resultantes tienen un promedio de $[num]%$ de las entradas originales, con algunos conjuntos
más afectados que otros.

[tabla de testset; con cantidad original y final de entradas]



- [3cosmul y 3cosadd, mencionar; ver dónde]
- presentación de resultados para cada embedding
    - no poner lista entera, sino los mejores 5 o algo así (o dos mejores por algoritmo)
    - comparar valores con resultados en inglés
    - mencionar, para analogías, el hecho de que se sacan comparatives que tienen alto porcentaje de accuracy (lo que hace que baje el promedio)
        - comparar sección a sección
    - ver qué embedding consigue mejores resultados por área (en similitud por ejemplo), etc.
- alguna discusión más:
    - comentar que los resultados son bastante comparables al inglés
    - mostrar algún ejemplo donde erre (capaz que eso de levy de errores por defecto? el paper de 3cosmul)
    - comparar con otros trabajos que hicieron lo mismo que nosotros de traducir vectores (billion word corpus, el italiano, polyglot, etc.; mejores resultados en español?)
    - discusión de 3cosadd vs 3cosmul (se corrobora lo mismo de levy, que es mejor el segundo)
    - discusión de resultados en top5 y top10 (``le erra pero está cerca'')
    - syntactic vs semantic
    - accents vs remove accents (casi no hay diferencia, aunque parece ser mejor dejarlos)


- disclaimer: el corpus afecta mucho el resultado como vio Levy, así que tomar las comparaciones con pinzas; además de los problemas de traducción


\section{Comportamiento Específico al Español}
(section name TBD)

- se arman testsets para hacer pruebas específicas del comportamiento en español, puesto que tiene mayor complejidad morfológica y hay más cosas para probar
- son todos testsets propuestos por nosotros, así que no hay comparación, pero está bueno para tener un poco más de insight de cómo se manejan
- los ooo van acá (todos, porque no son comparación con literatura)
- cuáles son y cómo se armaron los testsets:
    - detallar cuáles son los que se crearon (verbos, semantic field, noun gender, spain conjugations, etc.); comentar el tipo (analogy, ooo, etc.)
    - cómo se construyeron:
        - 150 verbos más comunes, scraping de conjugaciones, armado de testsets con selección random (mencionar scripts)
        - scraping de semantic fields (fue de wikipedia?)
        - lista de géneros para nouns (comentar que algunos se usan pero no son correctos, e.g. presidenta)
- tabla de testsets con cantidad de entradas y eso

- presentar resultados para cada embedding
    - para los no-spain, hacer como la vez pasada
    - alguna discusión más sobre los ooo, que hasta ahora no había
- discusión de resultados
    - (falta esperar a que se entrene)
    - distinguir por tipo de conjugación: hay algunas más simples que otras? (e.g. subjuntivo), puede ser por el corpus
    - ver resultados de spain-specific para evaluar composición del corpus
    - rever alguno de los puntos de la sección anterior
    - cantidad de ocurrencias de palabras por tiempo verbal (count de las palabras de las analogías en el corpus)


\section{Análisis Cualitativo de Resultados}

- estudiar resultados del most\_similar cualitativamente (tipo lo que hacen al final de paper de word2vec con russian river)
  (más informal)
- e.g. UTE, ANCAP y eso con corpus en español y corpus general; cómo se ``diluyen'' los significados (i.e. no recupera UTE o ANCAP en todo el corpus pero sí sólo en español, a pesar de que ANCAP sólo se menciona en uruguay (o no, pero buscar ejemplo donde sí en otro caso))
- algún ejemplo interesante

\chapter{Evaluación}

Contando ya con la herramienta y el corpus de texto en español, se procede a construir
representaciones vectoriales empleando los principales algoritmos en el área y a evaluar su
comportamiento en el idioma español. Con el fin de tener valores de referencia con los cuales
comparar los modelos entrenados, se traducen del inglés al español los conjuntos de pruebas más
utilizados en la literatura.

El capítulo comienza detallando el proceso de entrenamiento, enumerando cuáles son los modelos
vectoriales entrenados y qué hiperparámetros estos utilizan. Luego se expone la comparación de
dichos modelos con los resultados obtenidos para el inglés, presentando cuáles son los principales
conjuntos de pruebas empleados. También se realiza un estudio sobre la performance de los modelos
frente a pruebas diseñadas para evaluar particularidades del idioma español. Por último, se realiza
un análisis cualitativo de las representaciones, mostrando algunos ejemplos de su comportamiento de
manera más informal pero ilustrativa.


\section{Modelos Entrenados}

Como se mencionó anteriormente, se optó por entrenar representaciones vectoriales siguiendo los tres
principales enfoques de la literatura: modelos neuronales a través del algoritmo \texttt{word2vec},
modelos estadísticos mediante el empleo de una matriz PPMI con SVD, y modelos híbridos mediante el
algoritmo \texttt{GloVe}. El primero y el último utilizaron implementaciones ya existentes
(\texttt{Gensim} y la provista por los autores, respectivamente), mientras que el segundo fue una
implementación propia siguiendo los lineamientos de~\cite{Levy2015}.

Cada uno de estos modelos tiene distintos hiperparámetros que pueden afectar significativamente su
performance. En~\cite{Levy2015} los autores buscan unificar los hiperparámetros de estos modelos,
proponiendo un subconjunto de los mismos que contempla las tres variantes. El enfoque tomado en este
proyecto no llega a tal profundidad, pues quita parte de la versatilidad de algunos algoritmos, pero
intenta aún así presentarlos de manera unificada cuando es posible.

En el caso de \texttt{word2vec}, los hiperparámetros disponibles para modificar son:

\begin{itemize}

\item \textbf{Dimensión} (\texttt{dim}): dimensión de las representaciones inducidas (valor por
defecto: 300).

\item \textbf{Cantidad de ocurrencias mínima} (\texttt{min\_count}): cantidad mínima de ocurrencias
que una palabra debe tener para ser considerada parte del vocabulario (valor por defecto: 5). En
caso de no llegar a dicho límite, se excluye del corpus como si no estuviera presente.

\item \textbf{Ventana de contexto} (\texttt{win}): tamaño de la ventana de contexto utilizada hacia
cada lado de la palabra central (valor por defecto: 5). Notar que no se estudian variantes con
contextos asimétricos o con otras características.

\item \textbf{Subsampling} (\texttt{sub}): qué nivel de \textit{subsampling} realizar a las palabras
muy frecuentes, para evitar que sean sobrerrepresentadas en los vectores finales (valor por defecto:
0.00001). Hacer subsampling de una palabra implica removerla de una oración con una probabilidad
$p$\footnote{Se realiza subsampling si $\mathtt{sub} \times |V| > \#w$, donde $|V|$ es el tamaño del
vocabulario y $\#w$ la cantidad de ocurrencias de la palabra $w \in V$. En ese caso, el subsampling
se realiza con una probabilidad $p = 1 - \sqrt{\frac{\mathtt{sub}}{\#w}}$, siguiendo la sugerencia
de~\cite{Levy2015}.}.

\item \textbf{Algoritmo empleado} (\texttt{algo}): qué algoritmo de \texttt{word2vec} utilizar,
Skipgram (\texttt{sg}) o CBOW (\texttt{cbow}) (valor por defecto: \texttt{sg}).\ Como fue mencionado
en el capítulo 2, el primero busca predecir la palabra central en base a su contexto y el segundo lo
opuesto.

\item \textbf{Variante utilizada} (\texttt{var}): si construir y utilizar un softmax jerárquico para
la estimación de la probabilidad (\texttt{hs}) o si emplear la técnica de \textit{negative sampling}
(\texttt{ns}) (valor por defecto: \texttt{ns}).

\item \textbf{Cantidad de muestras negativas} (\texttt{neg}): en caso de utilizar negative sampling,
la cantidad de muestras negativas a tomar por cada ejemplo positivo (valor por defecto: 7).

\item \textbf{Ritmo de aprendizaje} (\texttt{alpha}): ritmo de aprendizaje a utilizar en el empleo
del descenso estocástico por el gradiente (SGD) mediante el cuál se aproximan las probabilidades
(valor por defecto: $0.025$). Un mayor valor para este parámetro corre el riesgo de que SGD no
converja, mientras que un menor valor puede hacer que se requieran de múltiples recorridas sobre el
corpus para obtener resultados aceptables. De todos modos, este valor es el ritmo inicial, e irá
disminuyendo conforme avance el entrenamiento para ayudar a la convergencia.

\item \textbf{Recorridas sobre el corpus} (\texttt{epoch}): cantidad de veces que se recorre el
corpus de texto (valor por defecto: 1). Recorrer un mayor número de veces el corpus de entrenamiento
mejora la convergencia del algoritmo pero, obviamente, hará que aumente el tiempo de entrenamiento
drásticamente.

\end{itemize}

El modelo \texttt{GloVe} presenta mucho de los mismos hiperparámetros (en particular, \texttt{dim},
\texttt{min\_count}, \texttt{win}, \texttt{alpha}, y \texttt{epoch}), con algunos hiperparámetros
adicionales:

\begin{itemize}

\item \textbf{Tamaño máximo del vocabulario} (\texttt{max\_count}): cantidad máxima de palabras que
puede contener el vocabulario donde, en caso de alcanzarse, se utilizan únicamente las
\texttt{max\_count} palabras más frecuentes (valor por defecto: \textit{sin máximo}).

\item \textbf{Cota de la función de peso} (\texttt{x\_max}): cota superior a utilizar para la
métrica de asociación entre los pares de las palabras en la matriz de coocurrencia intermedia (valor
por defecto: $100$). Si la métrica asume valores superiores a $100$, se toma el valor $100$ en su
lugar.

\item \textbf{Exponente para la función de peso} (\texttt{eta}): exponente a utilizar en la función
de peso, tal como es definido en~\cite{Pennington2014} (valor por defecto: $0.75$). Tanto este
parámetro como el anterior fueron dejados en sus valores por defecto en todos los modelos generados,
puesto que son muy particulares al algoritmo y su estudio escapa el alcance del proyecto. De todos
modos, como comentan los propios autores, su valor no debería ser determinante en la performance de
las representaciones resultantes.

\end{itemize}

Cabe notar, sin embargo, que el hiperparámetro \texttt{epoch} tiene una importancia mayor en este
algoritmo, pues no implica una recorrida por todo el corpus sino por la matriz de coocurrencias
intermedia que se genera (en particular, por el vocabulario empleado). \texttt{epoch} en este
contexto indica la cantidad de iteraciones empleada en la generación de la matriz reducida, y debe
ser un número alto para asegurar una correcta factorización. El valor por defecto es 15, aunque los
autores sugieren utilizar entre 50 y 100 iteraciones.

Por último, el modelo PPMI con SVD tiene también algunos hiperparámetros adicionales (\texttt{dim},
\texttt{min\_count}, \texttt{max\_count}, \texttt{win}, \texttt{sub}) con la incorporación de dos
adicionales:

\begin{itemize}

\item \textbf{Suavizado de la distribución del contexto} (\texttt{cds}): suavizado empleado para las
coocurrencias de las palabras en la construcción de la matriz de coocurrencia, tal como es detallado
en~\cite{Levy2015} (valor por defecto: $0.75$). Este parámetro disminuye el sesgo natural que la
medida PMI tiene con las palabras raras, e incrementa consistentemente la performance sobre la
opción sin suavizado (valor $1.0$). Por esta razón, en los modelos entrenados siempre se utilizó el
valor por defecto.

\item \textbf{Suma de vectores de contexto} (\texttt{w+c}): luego de aplicar SVD a la matriz de
coocurrencias, se obtiene una matriz que asocia las palabras del vocabulario al espacio reducido y
otra que asocia las palabras de los contextos al mismo espacio (valor por defecto:
\texttt{true}). En~\cite{Levy2015}, los autores proponen utilizar la suma de ambos vectores en lugar
de únicamente los vectores base, obteniendo en algunos casos mejores resultados.

\end{itemize}

Este último modelo tal como es planteado por los autores cuenta con algunos hiperparámetros
adicionales, como la normalización de los vectores resultantes y la variante de~\cite{Caron2001} en
SVD\@. Sin embargo, puesto que los resultados a los que llegan los autores sistemáticamente
favorecen a una variante particular, se optó por dejarlos fijos.


Por otro lado, también se cuenta con hiperparámetros para el preprocesamiento del corpus de texto
previo a ser alimentado a los algoritmos. En particular, se tienen dos opciones: \textbf{remover
tildes} (\texttt{acc}), que remueve todo tipo de tildes del texto previo al entrenamiento, y
\textbf{pasar a minúsculas} (\texttt{lower}), que pasa todo el texto a minúsculas, no haciendo
distinción entre \textit{Papa} y \textit{papa}.

También está la posibilidad de evaluar distintos mecanismos para la tokenización de oraciones y
palabras, pero se decidió limitar el estudio a los dos anteriores. En en el caso de la tokenización
de palabras, se partió las oraciones en secuencias de caracteres alfanuméricos, descartando todo
tipo de símbolo distinto.


La gran cantidad de hiperparámetros disponibles para cada algoritmo deja un espacio de búsqueda
extremadamente grande, que no es posible explorar por completo. Por este motivo se construyó sobre
el trabajo previo en la literatura, utilizando los valores de parámetros que mejores resultados han
obtenido y explorando en la proximidad de cada uno.

Procediendo de este modo, es posible obtener una mejor visión de cómo funcionan los distintos
algoritmos en su versión equivalente en español, replicando los comportamientos obtenidos en
inglés. Además, probando alternativas de los hiperparámetros no necesariamente estudiadas en la
literatura, se evita que se escapen variantes particulares o prometedoras.

Los valores por defecto de cada hiperparámetro son los que mejores resultados han obtenido en la
literatura, buscando que si se deja sin fijar uno de ellos, el modelo resultante sea aún así
razonablemente bueno. En el caso de \texttt{word2vec}, el modelo arquetípico utiliza la variante
Skipgram con negative sampling (utilizando 7 muestras negativas); establece una dimensión resultante
de 300, y una ventana de contexto de ancho 5; un subsampling de $1\times10^{-5}$, 5 ocurrencias
mínimas de una palabra para entrar al vocabulario y una única recorrida del corpus.

Para \texttt{GloVe} se parte de valores similares: 300 componentes en los vectores resultantes, al
menos 5 ocurrencias para ingresar al vocabulario, y una ventana de tamaño 5. No se limita el tamaño
máximo del vocabulario y, como se mencionó anteriormente, se utilizan los valores propuestos por los
autores del algoritmo para la cota y el exponente de la función de peso. Por último, se realizan 15
recorridas por la matriz de coocurrencias.

El modelo estadístico se basa también en los valores de hiperparámetros de los otros dos
algoritmos. Para el suavizado de la distribución de contexto se utiliza, como se explicó antes, un
valor de $0.75$ (y no se varió en ningún modelo), mientras que se realiza la suma de vectores de
contexto.

Algunos de estos hiperparámetros, como la dimensión de los vectores resultantes o el tamaño de la
ventana, dependen del tamaño del corpus que se utilice para entrenarlos, donde a mayores valores
funcionarán bien con más datos. De todos modos, los valores elegidos para éstos son razonables para
los volúmenes de texto que se manejan.

Por otro lado, hay hiperparámetros cuyos valores dependen de otros, como la cantidad de muestras
negativas en \texttt{word2vec} (que no aplica cuando se utiliza un softmax jerárquico). Otro ejemplo
son las limitantes al tamaño del vocabulario, donde \texttt{max\_count}, \texttt{min\_count}, e
incluso la dimensión y el tamaño de la ventana de contexto son utilizadas en conjunto para acotar
los requerimientos computacionales de correr los algoritmos.

Relacionado a este último punto, la cantidad de recorridas sobre el corpus en \texttt{word2vec} o de
la matriz de coocurrencias en \texttt{GloVe} tienen una influencia directa en el tiempo de cómputo
del que se disponga. Para este tipo de hiperparámetros, se tomaron valores que permitan entrenar
adecuadamente los modelos en el servidor del que se disponía, buscando mantener el tiempo de
entrenamiento para la mayoría de los vectores por debajo de un día.

Sobre estos valores base se entrenan a su vez alternativas, comparando los resultados al variar
hiperparámetros particulares, donde se mantiene el resto de los valores fijos. De esta forma se
busca estudiar la influencia que tiene, por ejemplo, la dimensión de los vectores o la cantidad de
recorridas por el corpus en la performance de las representaciones vectoriales.


En cuanto al corpus de entrenamiento empleado, se entrenaron dos grandes grupos de modelos
vectoriales: uno con un corpus de $4000$ millones de palabras de todos las fuentes (con la excepción
del texto de baja calidad obtenido de los foros) y otro de $400$ millones de palabras entrenado
exclusivamente con noticias de Uruguay, ambos con los tildes removidos. Se toman estas dos variantes
con el objetivo de estudiar cómo afecta el tamaño del corpus en la calidad de los vectores
resultantes. Estos grupos de modelos entrenados con texto genérico, a su vez, buscan ser comparables
con las representaciones que se entrenan en la literatura, los cuales utilizan corpus de texto que
tratan de no influir a los vectores de ninguna manera particular.

Además de estos dos conjuntos se entrenaron modelos adicionales con datos exclusivamente de España
(con alrededor de $400$ millones de palabras), con datos provenientes tanto de noticias como de
foros, con los datos anteriores pero sin los tildes removidos, y con la totalidad del corpus
obtenido finalmente ($6000$ millones de palabras). Estas variantes se pensaron con el fin de
estudiar cómo difiere el comportamiento de los vectores según el texto con el que se alimenta.

En la tabla [tabla] se listan todas las variantes de corpus entrenados, junto con un código para
identificarlos fácilmente, mientras que en la tabla [tabla] se listan todos los modelos vectoriales,
especificando los hiperparámetros empleados para cada uno y el corpus utilizado.

[tabla corpus: id, descripción específica (e.g. docs scrapeados antes de... (footnote con
explicación de por qué cualquier cosa)), cantidad palabras, preprocessing options][corpuses: full
con fecha, spain, uruguay, badquality/foros]

[tabla embeddings: id, descripción?, corpus utilizado, tiempo entr.; hiperparámetros; seccionar por
embedding type así puedo poner columna por hiperparámetro? poner todos?]


El entrenamiento de los modelos vectoriales fue realizado a través de la herramienta construida y
detallada en el capítulo 4. Los hiperparámetros a evaluar se fijaron a priori, dando de alta los
\textit{embeddings} en la interfaz web y mandando posteriormente a entrenar cada uno de ellos.

Como se puede apreciar en la tabla [tabla embeddings], se construyeron [num] representaciones
vectoriales distintas: [num] utilizando \texttt{word2vec}, [num] utilizando \texttt{GloVe}, y [num]
utilizando PPMI\@. El tiempo de entrenamiento promedio de cada modelo fue de [time2], donde el más
rápido tomó [time0] ([cualfue]) y el más lento [time1] ([cualfue]). Existe una diferencia
significativa entre el tiempo empleado en ambos extremos, lo que resalta la importancia de la
limitante que impone la capacidad de cómputo disponible al entrenamiento de vectores. En total, se
invirtió [time3] entrenando modelos vectoriales.

Cada modelo vectorial ocupa entre 300MB y 1000MB (con algunos llegando a 1700MB), dependiendo
principalmente de la dimensión elegida y los límites que se le establezcan al vocabulario (cantidad
mínima de ocurrencias, o directamente el tamaño máximo del vocabulario). Esto no genera grandes
presiones en el almacenamiento, aunque aún así no es un tema menor a la hora de descargar los
modelos o de cargarlos en memoria, pues tiene un tiempo de arranque no despreciable.


\section{Comparación con la Literatura}

Uno de los principales objetivos del proceso de evaluación es comparar la calidad de las
representaciones vectoriales generadas con texto en español con las generadas con texto en
inglés. Puesto que todos los conjuntos de pruebas son inherentemente dependientes al idioma de los
vectores, no es posible realizar una comparación perfecta, en especial debido a la fuerte
dependencia que tiene la calidad de los vectores con el corpus de entrenamiento empleado.

De todos modos, y para tener al menos un punto de comparación inicial entre ambos idiomas, se optó
por tomar los principales conjuntos de pruebas utilizados en la literatura inglesa y traducirlos al
idioma español, adaptando las pruebas cuando fuera necesario (por ejemplo, cuando se evalúan
cualidades sintácticas no existentes en español). Así, es posible obtener una referencia aproximada
de qué tanto se acercan los modelos entrenados a los obtenidos por la comunidad.

Se emplean dos tipos de pruebas para realizar la comparación: pruebas basadas en analogías y pruebas
basadas en similitud de palabras. Como ya fue mencionado en capítulos anteriores, la primera busca
recuperar una palabra en base a otras tres palabras dadas, mientras la segunda busca asignar un
puntaje de similitud entre pares de palabras.

Para las analogías existen dos conjuntos de pruebas muy utilizados por la comunidad: el conjunto de
analogías de Microsoft (MSR), compuesto exclusivamente por analogías que buscan regularidades
sintácticas en los vectores, y el conjunto de Google~\cite{Mikolov2013a}, compuesto tanto por
analogías sintácticas como semánticas. De estos dos, se optó por utilizar únicamente el de Google,
pues el MSR tiene demasiadas pruebas sintácticas específicas al inglés (como posesivos,
comparativos, y superlativos) que deben ser descartadas en el proceso de traducción, haciendo que el
conjunto resultante sea tan distinto que una comparación de los resultados sería insustancial.

Debido a la antigüedad del campo, existe una gran cantidad de conjuntos de pruebas para la tarea de
similitud de palabras, en especial en lo que respecta a la evaluación de modelos estadísticos. Se
decidió, para limitar la cantidad de conjuntos a un número razonable, elegir las pruebas más
recientemente utilizados por la literatura, con el fin de tener comparaciones más fiables. En
particular, se toman los conjuntos empleados por~\cite{Levy2015}, pues son evaluados con los tres
tipos de modelos empleados en el proyecto. Estos conjuntos son:

\begin{itemize}

\item WordSim~\cite{Finkelstein2002}, un conjunto compuesto por 353 pares de palabras puntuados en
base a la relación entre ambas palabras (si los jueces humanos consideran que las palabras están
relacionadas de alguna manera cualquiera) y su similitud (si existe una relación más fuerte entre
los pares, ya sea sinonimia, hiponimia, etc.). Se utiliza el conjunto partido entre los pares que
miden relación y los que miden similitud, como se hace en~\cite{Levy2015}.

\item MEN~\cite{Bruni2012}, compuesto por 3000 pares de palabras elegidas al azar entre las palabras
que aparecen al menos 700 veces en el corpus WaCky~\cite{Wacky2009}. Los autores utilizaron la
plataforma Mechanical Turk~\cite{MechanicalTurk} para la construcción del conjunto, y el puntaje se
asigna en base a comparaciones binarias entre entradas del conjunto.

\item El conjunto MTurk~\cite{Radinsky2011}, compuesto de 287 pares de palabras, con puntajes
también asignados a partir de la herramienta Mechanical Turk.

\item SimLex~\cite{Hill2014}, compuesto de 999 pares puntuados exclusivamente en base a la similitud
entre las palabras. Esto implica que pares como \textit{ropa} y \textit{ropero} tienen asignado un
puntaje bajo, por no ser estrictamente similares. Este conjunto es particularmente complicado de
resolver por parte de los modelos vectoriales, pues éstos aprenden en base a la coocurrencia de
pares y la coocurrencia refleja principalmente la relación que existe entre las palabras.

\end{itemize}

Por otro lado, se descarta el conjunto de palabras raras propuesto por~\cite{Luong2013} debido a que
la sutileza entre los pares de palabras empleados se perdería en el proceso de traducción.


Los cinco conjuntos de pruebas seleccionados para realizar la comparación debieron ser traducidos
manualmente, pues no fue posible encontrar traducciones satisfactorias de los mismos.

Se construyó un script que aprovecha el servicio de Google Translate~\cite{GoogleTranslate} para
realizar una traducción semi-automática de las pruebas. Este script lee una lista de palabras de
entrada, realiza un pedido HTTP al servicio y obtiene la primer traducción sugerida. Esta palabra se
toma posteriormente como una sugerencia que es revisada manualmente, pero que no obstante ayuda a
agilizar la tarea.

El proceso de traducción para cada conjunto contó, pues, de cuatro etapas: en primer lugar, se
corrió el script para la lista de palabras del conjunto; luego se recorrió la lista de palabras
traducidas, comparándolas con las versiones en inglés y se corrigieron o eliminan las traducciones
incorrectas, ya sea por estar mal la traducción en sí o porque no era el sentido que esperaba en la
prueba; en tercer lugar, se sustituyen las traducciones por las palabras originales en los conjuntos
de prueba, eliminando la entrada si no se encuetra la traducción; por último, se recorre la lista
pruebas buscando posibles errores y eliminando o corrigiendo entradas adicionales.

El proceso de traducción, sin embargo, es falible, por lo que es de esperar que existan traducciones
incorrectas en los conjuntos de pruebas, aunque la proporción no sea alta. Puesto que algunos
conjuntos de pruebas llegan a tener más de diez mil líneas, la revisión final fue más bien
superficial.

En el caso de las tareas de similitud de palabra, donde se encuentra frecuentemente diferencias
finas en el significado de los pares de palabras, se buscó que las traducciones siguieran la
intención de los autores. Por ejemplo, mientras que Google Translate sugería la traducción
\textit{costa} tanto para \textit{shore} como para \textit{coast}, se tomó la decisión de traducir
\textit{shore} como \textit{orilla}, que es una palabra también válida pero que se ajusta al puntaje
que el par tenía asociado. Esto indudablemente está sesgando el conjunto de pruebas, pero es la
única alternativa para realizar una comparación, fuera de construir los conjuntos de nuevo como
hicieron los autores originales.

En el conjunto de analogías de Google fue necesario eliminar por completo algunas secciones por no
ser pertinentes al idioma español. En particular, se eliminaron las categorías
\texttt{gram3-comparative} y \texttt{gram4-superlative}, que tienen analogías entre adjetivos y sus
comparativos o sus superlativos (e.g. \textit{great}, \textit{greater}, y \textit{greatest}), pues
no tienen traducciones al español de una única palabra. También se decidió eliminar la sección
\texttt{city-in-state}, que tiene analogías de ciudades y estados de EE.UU., pues nuestro corpus de
texto se basa en fuentes hispanas y no tendrá muchas menciones de las mismas.

En la tabla [tabla testsets] se resumen los conjuntos de pruebas empleados para la evaluación, junto
con la cantidad de entradas originales y resultantes de cada uno. Como se puede apreciar, las
pruebas resultantes tienen un promedio de [num]\% de las entradas originales, con algunos conjuntos
más afectados que otros.

[tabla de testset; con cantidad original y final de entradas]

\quad

La tabla [tabla] muestra los resultados obtenidos para los mejores dos modelos vectoriales de cada
algoritmo. También se presentan, como punto inicial de comparación, los resultados obtenidos en la
literatura.

[tabla resultados por grupos (Google total, wordsim, etc.)]

El primer punto a observar es que los resultados obtenidos por nuestros vectores son muy similares a
los obtenidos en inglés, particularmente en las tareas de analogías: los resultados de estado del
arte varían entre una exactitud de $0.61$ y $0.69$~\cite{Levy2015, Baroni2014, Pennington2014,
Mikolov2013c}, mientras que los vectores en español (particularmente, el modelo [modelname]),
obtiene $0.63$.

Parte de esta diferencia puede explicarse por el hecho de que las secciones que se remueven del
conjunto de pruebas (comparativos, superlativos, y ciudad en estado) son entradas que obtienen por
lo general buenos resultados. Este fenómeno puede observarse en~\cite{Levy2014b}, donde los autores
presentan los resultados desglosados por sección. Allí, el conjunto de comparativos, por ejemplo,
obtiene una exactitud de $0.86$.

En la tabla [tabla desgl] se muestran los valores obtenidos separados por sección del conjunto de
analogías, junto a los valores obtenidos por~\cite{Levy2014b}. Es interesante ver que el
comportamiento del mejor modelo es relativamente similar al inglés, incluso con las pruebas
sintácticas donde ambos idiomas, proveniendo de familias de lenguajes distintos, varían
significativamente. Por ejemplo, las secciones que peor performance obtienen son las mismas en ambos
idiomas: \texttt{currency}, \texttt{gram1-adjective-to-adverb}, \texttt{gram2-opposite}, todas muy
debajo del promedio. Por otro lado, también se repiten las que mejor funcionan:
\texttt{capital-common-countries}, \texttt{gram6-nationality-adjective},
\texttt{gram9-plural-verbs}, donde hay tanto categorías sintácticas como semánticas.

[tabla analogías desglosada]

El modelo vectorial vectorial que más éxito obtiene en analogías es [modelname], el cual utiliza el
algoritmo \texttt{word2vec} con Skipgram y \textit{negative sampling}, seguido de un modelo GloVe y
finalmente por un modelo PPMI/SVD\@. Esto sigue lo observado por los autores en~\cite{Levy2015},
aunque en ese caso la diferencia entre el modelo híbrido y el estadístico es mucho menor.

De todos modos, en el conjunto de pruebas MSR (el cual cuenta con analogías exclusivamente
sintácticas) los autores sí tienen una diferencia amplia entre ambos algoritmos, lo que puede
explicar este fenómeno; en especial si se nota que la mayor diferencia de aciertos en los vectores
en español se da en las secciones \texttt{gram8-plural-nouns} y \texttt{gram9-plural-verbs}: en
pruebas sintácticas. La diferencia en el conjunto de Google en particular podría explicarse por la
extracción de secciones durante el proceso de traducción.


Curiosamente, el comportamiento en el caso de similitud de palabras es opuesto: salvo por el
conjunto de pruebas [mturk], PPMI/SVD es consistentemente superior a las alternativas, con
los modelos \texttt{GloVe} y \texttt{word2vec} segundos con resultados muy parecidos. Esto también
es similar al comportamiento obtenido en~\cite{Levy2015}.

Algo que cabe resaltar, sin embargo, es que la diferencia entre los resultados en inglés y español
es mayor que para el caso de las analogías: mientras que las analogías obtenían aproximadamente un
$5\%$ de diferencia relativa en los resultados, estas pruebas llegan hasta un $25\%$ de diferencia
(en el conjunto SimLex, por ejemplo).

Esto último se puede explicar, en parte, porque los conjuntos de similitud de palabras son más
difíciles de traducir: el puntaje asignado es una noción muy intuitiva a la persona que los asigna,
algo que difícilmente se traslade de un par de palabras en inglés a su traducción en español. El
hecho de que el conjunto WordSim (entradas de similitud) y MEN obtengan resultados que van más
acorde al estado del arte, con aproximadamente un $5\%$ de diferencia relativa, podría explicarse
porque tienen entradas que resisten más el proceso de traducción (con menos sutilezas del
significado de las palabras).

Para obtener resultados equivalentes, sería necesario seguir un proceso similar a los creadores de
los conjuntos y buscar asignar puntajes a los traducciones de las palabras (así, por lo menos, es
equivalente el conjunto).


Vale la pena repetir que, como muestran los autores en~\cite{Levy2015}, la calidad de los vectores
resultantes está fuertemente ligada a la composición del corpus con que se entrenan, por lo que las
medidas presentadas de vectores en inglés, entrenados por otros autores, con otras implementaciones,
y otros corpus, deben limitarse a ser solamente una referencia. Esto es particularmente cierto en el
escenario actual por el hecho de estar además traduciendo los conjuntos de pruebas mediante un
proceso de traducción que, como ya se mencionó anteriormente, no está ni cerca de ser perfecto.


Otro punto interesante a comentar es que se logran reproducir los resultados obtenidos por los
autores en~\cite{Levy2014b}, donde se propone la medida \textsc{3CosMul} para la recuperación de
analogías. En todas las pruebas realizadas, esta medida obtiene resultados consistentemente
superiores a \textsc{3CosAdd}.


Una cuestión que no ha sido estudiado en profundidad en la literatura es, en los casos donde la
recuperación de la analogía falla, qué tan cerca se queda la respuesta. Con este objetivo se
calcula, además del porcentaje de aciertos en la primer posición (esto es, la palabra que maximiza
\textsc{3CosMul}), el porcentaje de entradas para las cuales la respuesta está entre las cinco o
diez palabras con mejor puntaje. En la tabla [tabla top1-5-10] se presenta esta información para los
modelos de antes.

[tabla top1-5-10]

Es interesante ver que los valores mejoran significativamente al considerar el top 5 y el top 10:
para el mejor modelo vectorial, se pasa de un $0.63$ a un $0.76$ en el top 5 y un $0.79$ en el top
10. Esto podría sugerir que con más cantidad de texto o con una mejor forma de recuperar analogías
se podrían obtener resultados aún mejores, porque los modelos cuentan con la información y no están
tan alejados de las respuestas correctas como parecería a priori: la sección
\texttt{gram1-adjective-to-adverb}, por ejemplo, pasa de un $0.22$ a un $0.48$, la mayor mejora de
todas las categorías.


Por último, es llamativo ver que, en contradicción a nuestra primera intuición, eliminar los tildes
tiene un efecto negativo sobre los resultados del corpus: en todos los casos donde se entrenaron
modelos con los mismos hiperparámetros salvo por la eliminación de los tildes, la versión que los
mantenía obtuvo mejores resultados en la mayoría de las pruebas.

Es posible que este fenómeno se dé por la dificultad que tienen estos algoritmos de distinguir
palabras polisémicas. Cuando una palabra puede tener más de un significado, el vector resultante
asociado será una mezcla de los vectores de cada uno de los sentidos de la palabra si se
intercambiaran por un \textit{token} propio\footnote{Esta mezcla, además, estará sesgada en base a
qué uso de la palabra es más frecuente: cuando uno es mucho más usado que otro, se corre el riesgo
de \textit{diluir} uno de los significados en el otro}.

Esto es, de hecho, lo que hacen los autores en~\cite{Huang2012,Tian2014}, donde construyen tokens de
la forma \textit{palabra\_1}, \textit{palabra\_2} para los significados 1 y 2. Así, incluir los
tildes de las palabras ayuda a distinguir entre las diferentes acepciones de las palabras, bajo la
suposición de que la cantidad de faltas de ortografía en el corpus de entrenamiento no es demasiado
elevada.

En la literatura existen también comparaciones de los algoritmos aquí presentados en distintos
idiomas, aunque con un nivel de profundidad menor. En~\cite{Berardi2015}, los autores realizan una
comparación de \texttt{word2vec} y \texttt{GloVe} con vectores entrenados en italiano con un corpus
compuesto de la Wikipedia (alrededor de 300 millones de palabras) y libros (alrededor de 2200
millones de palabras) en dicho idioma, realizando también una traducción de las analogías de Google
similar a la hecha en este proyecto. Consiguen, sin embargo, resultados significativamente
inferiores a los aquí presentados: entrenando con Wikipedia obtienen un acierto de $0.40$, mientras
que agregando libros $0.47$.

Los autores también corren las pruebas con Polyglot~\cite{AlRfou2013}, que son un conjunto de
modelos vectoriales en múltiples idiomas propuestos por sus autores como línea de base para
distintos experimentos, entrenados con las Wikipedias de cada lenguaje. Consiguen, sin embargo, una
exactitud en la recuperación de tan solo $0.04$, por lo que se decidió ni evaluar los vectores en
español.

En español se cuenta con los vectores presentados en~\cite{SBWCE}, donde el autor obtiene resultados
levemente inferiores a los nuestros con el conjunto de pruebas de Google traducido por
él\footnote{Dicha traducción no fue utilizada en el proyecto porque se publicó una vez que ya se
había traducido el conjunto de pruebas. Aún así, se realizó una comparación entre ambas traducciones
y se vio que la misma contenía algunos errores.}. No hay, sin embargo, una publicación adjunta que
describa el proceso de evaluación, por lo que se desconoce el detalle de las pruebas realizadas.


\section{Comportamiento Específico al Español}

Teniendo un punto de comparación de la performance de los modelos vectoriales en ambos idiomas, se
procedió a realizar un estudio más detallado de su comportamiento en español, con el objetivo siendo
obtener una mejor visión de qué tanta información son capaces de capturar los vectores en un idioma
con mayor compejidad gramatical, investigando cuáles son las características que resultan más
difíciles de representar.

Con este fin, se armaron conjuntos de pruebas que exponen cualidades del español, como conjugaciones
verbales, géneros de sustantivos, y más. Son conjuntos de prueba construidos para el presente
proyecto, por lo que no se tiene un punto de comparación con otros trabajos en la literatura. De
todos modos, consideramos que la información que aportan es valiosa, pues ayudan a tener un mejor
entendimiento de cómo actúan los modelos bajo diferentes circunstancias.


El primer grupo de pruebas construido está compuesto por analogías que buscan estudiar la capacidad
de representar las conjugaciones de los verbos. Se eligen doce formas verbales a evaluar
(e.g.\ participio, gerundio) y se arman grupos de analogías con el verbo en infinitivo y cada una de
estas formas verbales. Esto es, para el verbo \textit{correr} se arma, por ejemplo, una analogía de
la forma \textit{correr es a corriendo lo que cantar es a cantando} y una de la forma
\textit{corrido es a correr lo que jugado es a jugar}.

Para la construcción de dicho conjunto, se eligieron los 147 verbos más comunes del
español\footnote{Acorde a [cita].}, se obtuvieron sus conjugaciones\footnote{Las conjugaciones se
consiguieron mediante el scraping de [cita].}, y se generaron un grupo de analogías para cada una de
las doce formas verbales. Estos grupos se armaron tomando, para cada verbo, otros diez verbos de la
lista y construyendo analogías de la forma \textit{verbo\_infinitivo\_1 es a verbo\_conjugado\_1 lo
que verbo\_infinitivo\_2 es a verbo\_conjugado\_2}, o en el orden inverso, con el conjugado primero.

Esta tarea se realizó con la ayuda de un script que facilitó la tarea de conjugar y elegir verbos
aleatoriamente, asegurando una distribución no sesgada en el conjunto de pruebas. Así, se obtuvieron
1470 analogías por cada grupo, teniendo un total de 17640 analogías dedicadas al estudio de
conjugaciones verbales.


Buscando estudiar la relación de los vectores con el corpus utilizado para el entrenamiento, también
se generaron analogías utilizando conjugaciones verbales específicas al Río de la Plata (en
particular, la conjugación de \textit{vos}, o \textit{voseo}), con el fin de ver la performance de
los vectores en estas pruebas cuando se entrena con un corpus exclusivamente rioplatense, uno
exclusivamente español, o uno global. La construcción de dicho conjunto de pruebas fue igual al
recién detallado, donde se toma una única forma verbal en lugar de doce.


El último conjunto de analogías construido estudia el género de sustantivos. Más específicamente,
toma ocupaciones (como \textit{plomero}, \textit{maestro}, o \textit{doctor}) y genera analogías de
la forma \textit{doctor es a doctora lo que pescador es a pescadora}. Para su construcción, se tomó
una lista de 78 ocupaciones en sus formas masculina y femenina\footnote{La lista fue tomada de
[cita].} y se generaron analogías en un esquema similar a los anteriores, donde cada ocupación fue
emparejada con otras seis ocupaciones, teniendo así un total de 468 entradas.


Se construyeron, por último, dos conjuntos de pruebas siguiendo el esquema \textit{odd-one-out}
descrito en el capítulo anterior. El primero no estudia fenómenos particulares del español, sino que
tiene un enfoque semántico: busca identificar la palabra que no encaja (desde el punto de vista de
su significado) con un conjunto de varias palabras. Para ello, se obtuvieron listas de palabras por
campo semántico\footnote{Obtenidas mediante [cita].}, como colores, frutas, y vehículos, y se
generaron combinaciones con tres o cuatro palabras de un campo particular y una de otro. Se
construyó un conjunto con un total de 529 entradas con estas características, obteniendo así una
forma de probar la capacidad de los vectores de agrupar palabras similares sin tener que recurrir a
una prueba que asigne puntajes (a veces arbitrarios) a pares de palabras, como en el caso de las
pruebas de similitud antes mencionadas.

El segundo conjunto de pruebas de este esquema tiene las mismas características pero propone pruebas
sintácticas en lugar de semánticas. Para ello, se generaron entradas utilizando la misma lista de
147 verbos y se armaron diez pruebas para cada uno de ellos. Se tomaron cinco formas verbales y se
generaron así cinco subconjuntos de pruebas con 294 entradas cada una, obteniendo un total de 1470
entradas.

En la tabla [tabla testsets] se listan todos los conjuntos de pruebas construidos en esta sección,
junto a la cantidad de entradas de cada uno y cuál es su propósito (i.e.\ qué área buscan evaluar).

[tabla de testsets con cantidad de entradas y qué busca evaluar (e.g. presente 3era persona singular, etc.)]

\quad

En la tabla [tabla resultados] se muestran los resultados a los que llegan las mejores dos
representaciones vectoriales de cada algoritmo. En el caso de los conjuntos sintácticos, se dividen
también por conjugación verbal.

[tabla resultados]

El primer punto a notar es que en el caso de las analogías los mejores resultados son similares a
los obtenidos en la sección anterior, con las pruebas de ocupaciones levemente superiores y las de
verbos algunos puntos por debajo. El algoritmo que mejor funciona en la tarea de analogías es
nuevamente \texttt{word2vec}, con \texttt{GloVe} en segundo lugar y PPMI/SVD en tercero.

Observando los valores, es posible ver que los resultados de las pruebas varían significativamente
entre una conjugación y otra: mientras los gerundios (\texttt{AV04}) llegan a un acierto de $0.84$,
el futuro subjuntivo singular (\texttt{AV11}) obtiene sólo $0.08$. Esto parece razonable, pues una
conjugación es mucho más usada que la otra, y los algoritmos aprenden mejores representaciones para
palabras más comunes\footnote{Esto es especialmente cierto en el lenguaje empleado en las noticias,
que abarcan la mayor parte del corpus.}.

De hecho, uno de los principales problemas con el conjunto \texttt{AV11} es que muchas de las
palabras que lo componen no se encuentran ni siquiera en el vocabulario del mejor modelo vectorial
(ver tabla [tabla missingwords]), lo que hace que casi la tercera parte de las entradas no puedan
ser respondidas correctamente.

[tabla missing words: para mejor algoritmo, missing words per verbtest; count; missing entries]

Es de esperar, por lo tanto, que si se obtiene un corpus con una mejor distribución de conjugaciones
verbales, el resultado de las pruebas mejore significativamente. En la tabla [tabla timeposv] se
presenta la distribución de los distintos tiempos verbales en el corpus de entrenamiento, derivado
en base a la ocurrencia en el texto de las palabras utiliazadas en el conjunto de analogías
verbales. Como es de esperar, las conjugaciones que mejores resultados obtienen son las más
frecuentemente empleadas enel corpus.

[tabla cantidad de ocurrencias de palabras por tiempo verbal (count de las palabras de las analogías
en el corpus)]

Siguiendo la discusión de la sección anterior respecto al uso de tildes en el corpus, es interesante
ver que en el conjunto \texttt{AV07} (conjugaciones en pretérito simple, tercera persona del
singular) el resultado obtenido por el modelo vectorial que mantiene los tildes es
significativamente superior al que no usa, presentando una mejora de un $32\%$.

Esto refuerza la idea de que los tildes ayudan a desambiguar palabras que de otra forma serían
polisémicas. En el caso de esta conjugación verbal, permite distinguir entre \textit{cambió} y
\textit{cambio}, \textit{cortó} y \textit{corto}, o \textit{sacó} y \textit{saco}. De hecho, la
única otra conjugación verbal para la cual hay una diferencia grande entre ambas modalidades de
preprocesamiento es en \texttt{AV11} (subjuntivo futuro, singular), pues es la otra donde hay
colisiones de palabras al remover los tildes: \textit{apagare} y \textit{apagaré}, \textit{volare} y
\textit{volaré}, o \textit{tocare} y \textit{tocaré}. Las otras conjugaciones que utilizan tildes,
como \texttt{AV01} (condicionales), no pasan a ser otra conjugación al removerlos.

Es interesante ver que el aumento en los aciertos en el top 1 y el top 10 es mucho mayor para el
caso de los verbos (pasando de $0.54$ a $0.77$ en el mejor modelo) que en el caso de las ocupaciones
(que va de $0.63$ a $0.78$). Esto podría explicarse porque en el caso de los verbos es más posible
que se confunda la palabra con distintas conjugaciones de la misma (que muchas veces pueden
utilizarse en contextos similares), lo que más difícilmente ocurre en el caso de ocupaciones (por
haber dos ejes para variar: género y número).


En cuanto a las pruebas del tipo \textit{odd-one-out}, los resultados obtenidos son muy elevados,
con el $90\%$ y $80\%$ de las respuestas siendo correctas en las pruebas semánticas y sintácticas,
respectivamente. El mejor algoritmo vuelve a ser \texttt{word2vec}, pero en el caso de la prueba
semántica, \texttt{O01}, PPMI/SVD está en un cercano segundo lugar.

En las prueba sintáctica (\texttt{O02}), sin embargo, PPMI/SVD consigue resultados bastante
inferiores a los otros modelos, con \texttt{GloVe} en un segundo lugar. Este punto sigue apoyando la
noción de que el modelo estadístico captura mejor las relaciones semánticas entre las palabras que
las relaciones sintácticas, pues vuelve a sobresalir en este escenario al igual que lo hizo en las
tareas de similitud de palabras.

Este último punto es importante, porque parecería indicar que PPMI/SVD logra capturar únicamente
relaciones de más alto nivel, donde las relaciones de coocurrencia son más fuertes, y no las
sutilezas entre usos de palabras menos frecuentes, como distintas conjugaciones de verbos en las que
cada variante aparece una menor cantidad de veces. Esto coincide con las conclusiones
de~\cite{Levy2014a}, en la que los autores plantean que la principal fortaleza de \texttt{word2vec}
(específicamente, de Skipgram con negative sampling) radica en la capacidad que tiene este algoritmo
de no sobrerrepresentar palabras demasiado comunes, distribuyendo la información en los vectores de
manera más uniforme.


En la tabla [tabla rp-spec] se muestran los resultados de las evaluaciones con el conjunto de
conjugaciones verbales específicas del Río de la Plata. Es interesante ver cómo los modelos
vectoriales que son entrenados exclusivamente con texto de foros argentinos obtienen mejores
resultados que los entrenados con la totalidad del texto. En los modelos vectoriales grandes hay
también muchas ocurrencias del \textit{voseo} (principlamente viniendo de los sitios de literatura
amateur), pero proporcionalmente es mayor la cantidad en los modelos construidos a partir de foros,
por tener un tono más informal y conversacional.

Este último punto vuelve a resaltar la importancia de la composición del corpus de entrenamiento,
además de indicar una especie de dilución de los vectores de palabras cuando la ocurrencia de las
mismas se encuentra limitada a una sección del corpus completo (en este caso, el uso del
\textit{voseo} a foros y sitios de literatura amateur, y no tanto en noticias).

[tabla rp-spec]


\section{Análisis Cualitativo de Resultados}

En las secciones anteriores se presentó un análisis formal basado en la evaluación de vectores con
características variadas bajo una serie de conjuntos de prueba común. Se realizó además
una comparación con la literatura existente para el caso del inglés. En esta sección se presenta
un estudio más intuitivo de los resultados obtenidos a través de una serie de ejemplos con
características interesantes.

Los ejemplos mostrados a continuación fueron generados utilizando dos modelos diferentes, ambos
entrenados utilizando \texttt{word2vec}. En uno de los casos se entrenó utilizando un corpus
compuesto de documentos de todas las fuentes recopiladas hasta principios de noviembre de 2015,
totalizando aproximadamente unos cuatro mil millones de palabras (\textit{Full word2vec}). En el
otro caso se utilizaron solamente documentos uruguayos con fecha no más reciente que octubre de
2015, alcanzando un total de 475 millones de palabras (\textit{Reduced word2vec}).

Para el análisis cualitativo se realizaron dos tipos de ensayos: analogías y palabra que no encaja. Las
analogías son del estilo \textit{peso es a Uruguay lo que rupia es a India}. En términos vectoriales
se puede expresar como $rupia \approx india + (peso - uruguay)$. El segundo ensayo consiste en
detectar que \textit{amarillo} es la palabra que no encaja en la lista \textit{saturno, marte, venus,
amarillo, urano}.

Comenzamos presentando en los cuadros \ref{table:analogies_presidents} y
\ref{table:analogies_currencies} analogías relacionadas a presidentes y monedas de países. En ambos
casos se utilizó el modelo \textit{Full word2vec} y como puede apreciarse el modelo se comporta de
forma deseable.

En el caso de presidentes se parte de la relación de Uruguay con el expresidente José Mujica y tanto
para Brasil como para España se obtienen sus actuales primeros mandatarios, Dilma Rousseff y
Mariano Rajoy, y también exmandatarios como son Luiz Inácio Lula da Silva y José Luis Rodríguez
Zapatero. En el caso de Argentina se obtienen tanto el nombre como el apellido de la expresidenta
Cristina Fernández de Kirchner.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (uruguay, mujica, brasil) & (uruguay, mujica, espana) & (uruguay, mujica, argentina)\\
        \hline
        dilma, 0.68 & zapatero, 0.54 & kirchner, 0.64\\
        rousseff, 0.67 & rajoy, 0.54 & cristina, 0.62\\
        lula, 0.66\\
        \hline
    \end{tabular}
    \caption{Analogías para países y presidentes.}
    \label{table:analogies_presidents}
\end{table*}

Para el caso de analogía de monedas también se obtienen resultados satisfactorios. Se parte de
la relación entre Uruguay y peso y se obtienen las monedas actuales o recientes de España, Japón
e India. En el caso de India, se obtiene también taka, moneda oficial de Bangladesh.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (uruguay, pesos, espana) & (uruguay, pesos, india) & (uruguay, pesos, japon)\\
        \hline
        pesetas, 0.59 & rupias, 0.60 & yenes, 0.58\\
        euros, 0.50 & takas, 0.48 & yens, 0.50\\
        \hline
    \end{tabular}
    \caption{Analogías para países y monedas.}
    \label{table:analogies_currencies}
\end{table*}

En el cuadro \ref{table:analogies_football} se presenta un ejemplo relacionado al fútbol. Se
parte de la relación entre el club español Valencia CF y su estadio Mestalla, y se obtiene tanto
como para el club español Real Madrid y el club argentino Boca Juniors los nombres de sus
respectivos estadios; Santiago Bernabéu y Bombonera. Se explora también cómo se
comporta el modelo cuando se le presentan conceptos relacionados a dos clubes de gran
rivalidad deportiva. Se parte entonces de la relación del club Real Madrid y su exentrenador
José Mourinho y se obtiene para el FC Barcelona el nombre de su también exentrenador Pep
Guardiola.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (valencia, mestalla, madrid) & (valencia, mestalla, boca) & (madrid, mourinho, barcelona)\\
        \hline
        bernabeu, 0.79 & bombonera, 0.60 & guardiola, 0.80\\
        madridista, 0.74 & xeneize, 0.55 & cule, 0.74\\
        \hline
    \end{tabular}
    \caption{Analogías relacionadas al fútbol.}
    \label{table:analogies_football}
\end{table*}

Se buscó también ver los resultados proporcionados por el modelo al explorar la relación
ente compañías y sus principales productos. En el cuadro
\ref{table:analogies_companies_products} se parte de la relación entre Volkswagen, fabricante
alemán de automóviles, y la palabra autos y se obtienen interesantes resultados para las
compañías Sony, Apple y Zara.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (volkswagen, autos, sony) & (volkswagen, autos, apple) & (volkswagen, autos, zara)\\
        \hline
        dvds, 0.67 & iphones, 0.65 & tiendas, 0.52\\
        lcds, 0.62 & ipods, 0.64 & accesorios, 0.51\\
        consolas, 0.62 & smartphones, 0.63 & zapaterias, 0.51\\
        \hline
    \end{tabular}
    \caption{Analogías relacionadas a marcas y sus productos.}
    \label{table:analogies_companies_products}
\end{table*}

Se realizaron también algunas pruebas con analogías de personajes de ficción. En el cuadro
\ref{table:analogies_fiction} se puede ver la relación entre nombres y apellidos
(o nombre compuesto) de personajes y la relación entre el título de un libro y el apellido de su
autor. Se puede ver como la relación entre Darth y Vader ayuda a construir el nombre Frodo
Baggins. En el caso de Harry Potter y Ron Weasley podemos observar que el modelo se
confunde y relaciona la palabra ron con su acepción como bebida alcohólica, sugiriendo por
tanto la marca de ron Bacardi.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (darth, vader, frodo) & (harry, potter, ron) & (harry, potter, hermione) & (anillos, tolkien, potter)\\
        \hline
        baggins, 0.65 & bacardi, 0.59 & weasley, 0.60 & harry, 0.69\\
        hobbit, 0.62 & harpper, 0.59 & granger, 0.57 & rowling, 0.63\\
        elijah, 0.60 & dennis, 0.58 & grint, 0.55 & hallows, 0.58\\
        \hline
    \end{tabular}
    \caption{Analogías relacionadas a personajes y autores de ficción.}
    \label{table:analogies_fiction}
\end{table*}

Como ya se comentó, se realizaron también pruebas de palabra que no encaja. Se presentan
a continuación los resultados del modelo \textit{Full word2vec} cuando se mezcla un político
entre escritores, un político uruguayo entre políticos europeos y un futbolista inglés entre
futbolistas brasileños. Los resultados en estos casos fueron todos satisfactorios.

\begin{lstlisting}
  >>> full_word2vec.doesnt_match(['rowling', 'tolkien', 'cortazar', 'obama'])
  <<< 'obama'
\end{lstlisting}

\begin{lstlisting}
  >>> full_word2vec.doesnt_match(['cameron', 'putin', 'mujica', 'merkel', 'rajoy'])
  <<< ‘mujica’
\end{lstlisting}

\begin{lstlisting}
  >>> ffull_word2vec.doesnt_match(['ronaldinho', 'neymar', 'dunga', 'rooney', 'robinho'])
  <<< ‘rooney’
\end{lstlisting}

También resultó interesante explorar analogías relacionadas estrictamente con Uruguay,
utilizando el modelo \textit{Reduced word2vec}. Se incluyen en el cuadro
\ref{table:analogies_uruguay_1} analogías relacionadas a entes y figuras estatales de
Uruguay. En el cuadro \ref{table:analogies_uruguay_2} se presentan analogías referentes
al fútbol uruguayo y algunas de sus figuras más relevantes.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (agua, ose, energia) & (ose, agua, ancap) & (interior, bonomi, ancap)\\
        \hline
        ute, 0.71 & petroleo, 0.57 & coya, 0.62\\
        ruchansky, 0.67 & hidrocarburos, 0.55 & sendic, 0.58\\
        \hline
    \end{tabular}
    \caption{Analogías relacionadas a entes públicos uruguayos.}
    \label{table:analogies_uruguay_1}
\end{table*}

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (manya, penarol, bolso) & (pacheco, penarol, recoba) & (defensor, franzini, river)\\
        \hline
        tricolores, 0.51 & danubio, 0.68 & plate, 0.67\\
        bolsos, 0.50 & tricolor, 0.68 & saroldi, 0.65\\
        \hline
    \end{tabular}
    \caption{Analogías relacionadas al fútbol uruguayo.}
    \label{table:analogies_uruguay_2}
\end{table*}

Finalmente, se logró comprobar que, en algunos casos, las analogías referentes a
Uruguay que funcionan muy bien en el modelo reducido se pierden cuando se
las evalúa en el modelo completo. En el cuadro \ref{table:analogies_uruguay_3} puede observarse el
comportamiento en el modelo \textit{Full word2vec} de algunas de las analogías evaluadas
satisfactoriamente en el modelo \textit{Reduced word2vec}.

\begin{table*}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        (manya, penarol, bolso) & (ose, agua, ancap)\\
        \hline
        mochila, 0.61 & cual, 0.47\\
        maleta, 0.61 & estalactita, 0.46\\
        bolsito, 0.60 & liquido, 0.46\\
        \hline
    \end{tabular}
    \caption{Analogías relacionadas a Uruguay con modelo completo.}
    \label{table:analogies_uruguay_3}
\end{table*}

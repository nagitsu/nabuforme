\chapter{Corpus}

El tamaño del corpus de texto utilizado para entrenar las representaciones vectoriales está
fuertemente ligado a la performance que éstas consiguen en las principales tareas de evaluación,
como se detalló en el capítulo anterior. Esto genera la necesidad de construir un corpus de texto en
idioma español del mayor tamaño posible, buscando que sea comparable con los que se manejan en la
literatura en inglés.

El capítulo empieza por presentar el relevamiento realizado sobre el tamaño y calidad de los corpus
en español existentes. Luego se plantean los requerimientos y características que se consideró que
el corpus debía contener, y se da una visión general del proceso empleado para lograrlo. Finalmente,
se detalla la implementación de dicho esquema y se resumen las características del coprus
construido.

\section{Estado del Arte}

Como ya se mencionó, una de las claves principales para la calidad de los vectores generados es contar
con un corpus de gran tamaño, compuesto de documentos con estilos de redacción y temáticas variadas.
Se procedió por tanto a investigar la oferta existente de corpus en idioma español en Internet. En esta
sección se discutirán los proyectos de construcción de corpus más relevantes, detallando de cada uno su
composición, medidas de calidad y limitantes de acceso ó uso, en caso de haberlas.

La investigación incluida en esta sección no pretende ser completamente exhaustiva, pero si lo
suficientemente profunda como para justificar la decisión de construir un nuevo corpus en lugar de
utilizar alguno ya existente para el entrenamiento de vectores. Como se verá, muchos de los
proyectos de construcción de corpus existentes no cumplen con las condiciones más importantes
requeridas por el presente proyecto.

Se procedió entonces a realizar una investigación formal de las alternativas existentes. Una de las
fuentes consultadas fue la investigación realizada en el año 2009 por el Centro Virtual Cervantes [CITA3].
En su trabajo los autores dan cuenta de la existencia de diversos proyectos para la construcción de corpus
en español clasificándolos en distintas categorías, entre ellas el tamaño de los corpus construidos y la
capacidad de acceso a los textos que forman los mismos. Estás características son, naturalmente, muy
relevantes para la tarea de construcción de vectores de palabras. A pesar de su considerable valor, la
investigación realizada por los autores apunta a corpus de muy diversos tipos, incluyendo grabaciones
de entrevistas orales, y no resultó ser una referencia definitiva para nuestra investigación.

Aún así, la investigación del Instituto Virtual Cervantes resultó un buen punto de partida para nuestro
relevamiento. Resulta interesante destacar que los autores obtienen entre sus principales conclusiones
que la mayoría de los trabajos analizados contaron, en alguna medida, con apoyo económico de entidades
privadas o gubernamentales para su desarrollo. Consideramos que este hecho pone en perspectiva los logros
obtenidos en el presente proyecto de grado para la construcción de un corpus masivo, cuyos detalles se
explicarán en las secciones posteriores.

\subsection{Análisis de proyectos existentes}

A continuación se incluye un listado de los corpus encontrados en nuestra investigación y un análisis de
las principales características de cada uno.

\subsubsection{Wikipedia}

Como punto inicial es interesante considerar el corpus formado por todos los artículos de Wikipedia en
español. En los últimos años, numerosos trabajos en el área del PLN han utilizado exitosamente los
textos de Wikipedia para sus algoritmos [CITA1]. Entre sus principales ventajas se destaca la calidad
de sus textos, la enorme variedad de temáticas abarcadas por los mismos y, para nada menos importante,
la naturaleza abierta y de libre acceso que diferencia a Wikipedia de otras fuentes de datos.

En idioma inglés, Wikipedia contiene casi tres billones de palabras repartidas en aproximadamente
cinco millones de artículos [Cita2]. En idioma español la cifra es mucho menor, ubicándose en 400 millones
de palabras. Por tanto, quedó claro desde un principio que utilizar un corpus constituido únicamente
por artículos de Wikipedia no sería suficiente para nuestros objetivos, aunque si resultaría de gran
utilidad para comenzar a construir un corpus de mayor tamaño.

\subsubsection{Sketch Engine: TenTen}

Se consideró también el trabajo realizado por la organización Sketch Engine [cita], dedicada a la
construcción de corpus masivos en varios idiomas. Además, la organización elabora herramientas para
la manipulación y consulta de los corpus generados, pensadas para profesionales de distintas áreas.
Entre los corpus ofrecidos por Sketch Engine se destaca principalmente el proyecto TenTen [cita].
TenTen consiste en una colección de corpus repartidos en varios idiomas superando en varios de ellos la
barrera del billón de palabras. Para el caso concreto del español se tiene un corpus de ocho billones de
palabras. El corpus TenTen es catalogado por sus autores como un corpus web, lo que implica que para
su construcción se utilizaron únicamente textos disponibles en Internet.

Existen diferentes estrategias para la construcción de corpus basados en Internet. En el caso de TenTen
los autores utilizaron un proceso basado en crawling masivo, limpieza de documentos, detección de
duplicados y almacenamiento. Para esto, los autores construyeron varias herramientas que posteriormente
liberaron como código abierto.

Para la tarea de crawling masivo en la construcción de TenTen se utilizó la herramienta Spiderling
[CITA4], desarrollada especialmente para la construcción de corpus utilizando Internet. En la experiencia
de los autores, el principal problema de los corpus web es la enorme cantidad de páginas que es necesario
descargar para obtener una cantidad relativamente pequeña de texto útil. En su análisis definen la
medida \textit{tasa de rendimiento} (\textit{yield rate}), que se aplica a cada página web descargada y
expresa la relación entre la cantidad de bytes de la página y la cantidad de bytes de información útil
obtenida a partir de la misma:

\[
  \textit{tasa de rendimiento} = \frac{\textit{tamaño de datos útiles obtenidos}}{\textit{tamaño total de la página}}
\]

Spiderling buscará de esta manera priorizar dominios que presenten tasas de rendimiento altas en el
conjunto de sus páginas, permitiendo una extracción inicial a modo de muestra para decidir luego si
vale la pena continuar procesando el dominio o si es mejor descartarlo. De este modo los autores aseguran
que se obtiene un ahorro importante de ancho de banda y de tiempo de limpieza sin apenas comprometer
el tamaño final que tendrá el corpus generado.

Naturalmente, luego del proceso de crawling es necesario extraer el texto útil a partir del código
HTML descargado. En el caso de TenTen, los autores utilizaron herramientas construidas por ellos mismos
para eliminar etiquetas HTML y texto \textit{boilerplate} [CITA5]. La definición de este último concepto
es difícil y suele depender del contexto de su aplicación. En nuestro caso consideramos como
\textit{boilerplate} aquel texto que refiere a la estructura y navegación de una página web y no a su
contenido real. Por ejemplo, texto de enlaces de una barra de navegación, texto perteneciente a secciones
de publicidad, metadatos de la estructura del sitio, etcétera.

Lamentablemente, los corpus generados por Sketch Engine no están disponibles de forma gratuita. La
organización ofrece en su sitio web un período de prueba sin costo para acceder a los mismos, pero la
utilización prolongada de sus servicios requiere una suscripción. Esta modalidad de uso, además de estar
por fuera del alcance de este proyecto de grado, entra en conflicto con uno de nuestros principales
objetivos como es la publicación total del corpus construido para el entrenamiento de vectores de palabras.

\subsubsection{Linguistic Data Consortium: Gigaword en español}

Otro corpus que vale la pena analizar es el corpus Gigaword en español construido por el Linguistic Data
Consortium (LDC) [CITA]. El LDC es un consorcio de universidades, bibliotecas y laboratorios de
investigación gubernamentales creado para enfrentar el problema de escasez de datos en las áreas de
investigación vinculadas al lenguaje.

El corpus Gigaword, en su tercera edición, cuenta con más de un billón da palabras repartidas en casi
cuatro millones de documentos almacenados en formato SGML/XML [CITA6]. Los documentos utilizados para
construir el corpus son en su totalidad cables de noticias provistos por las agencias internacionales
Agence France-Presse, Associated Press (AP) y Xinhua News Agency.

Los documentos del corpus fueron catalogados por los autores, aunque estos advierten que las categorías
asignadas son simplemente aproximaciones. Dichas categorías no son de mucha utilidad pues apenas
distinguen entre reportes, como los que pueden encontrarse en cualquier revista de noticias, y cables
dirigidos a editores y reporteros de periódicos. En cuanto a la calidad, los autores sólo aseguran
controles exhaustivos en el formato de los datos, eliminando errores de recepción que corrompan el
texto original, pero nada más. Se realiza también un proceso de filtro de cables repetidos, aunque se
menciona que dicho proceso no es totalmente riguroso. Los autores advierten que es probable encontrar
algunos errores de redacción o de ortografía, típicos del ritmo y exigencias diarias que se dan en los
periódicos y agencias de noticias.

Como ocurre con TenTen el acceso a Gigaword no es gratuito, salvo para miembros del LDC. Los costos de
adquisición del corpus resultan prohibitivos para el alcance del presente proyecto y, como ya se mencionó,
esta modalidad de distribución tampoco se alinea con los objetivos del mismo.

\subsubsection{Universidad de Brigham Young: Corpus del Español}

El Corpus del Español es un proyecto elaborado por Mark Davies del departamento de lingüística de
la Universidad de Brigham Young y financiado a través de una subvención del fondo nacional
estadounidense \textit{US National Endowment for the Humanities} [CITA7]. Este corpus está compuesto
por cien millones de palabras, la cual es una cantidad relativamente pequeña si la comparamos con las
cantidades de los proyectos ya analizados. Sin embargo,  este corpus cuenta con la particularidad de
contener documentos de bastante antigüedad, abarcando el período desde el siglo XIII al siglo XX.
Recientemente, el proyecto recibió una beca para extender el tamaño del corpus a dos billones de
palabras. Dicha extensión comenzó a realizarse en 2015 y continuará durante el año 2016.

En la descripción de su trabajo, Davies explica los motivos para construir este corpus en español,
habiendo ya alternativas disponibles de mucho mayor tamaño. Su argumento principal es que el tamaño
no lo es todo y resulta muy importante contar con un corpus correctamente anotado y etiquetado para
que este pueda ser de utilidad en la mayoría de las tareas. En su crítica a otros proyectos, el autor
argumenta que en la mayoría de los casos se aprecia un bajo conocimiento del idioma español por parte
de los encargados de los mismos y que el etiquetado de los corpus publicados fue hecho de manera
automática sin prestar demasiada atención al resultado final.

Curiosamente, y a pesar de que los argumentos de Davies no deben ser despreciados, en el caso de
entrenamiento de vectores de palabras no es necesario contar con un corpus anotado y etiquetado, como
lo es en una gran cantidad de otras aplicaciones.

Finalmente, el acceso al Corpus del Español es totalmente gratuito, aunque no cuenta con mecanismos
explícitos para descargarlo en su totalidad. Además, y de forma entendible, las consultas al corpus
están limitadas de forma diaria por usuario registrado, y por dirección IP para usuarios anónimos.
Estas limitantes de acceso, sumadas a las características ya descritas, hacen que este corpus no sea
de gran utilidad para el presente proyecto.

\subsubsection{Corpora from the Web: ESCOW14}

El proyecto Corpora from the Web (COW) es una iniciativa del grupo de gramática y lingüísticas
generales de la Universidad Libre de Berlín (Freie Universität Berlin) [CITA]. El proyecto consta
de una colección de corpus web en varios idiomas, entre los que se encuentra el español. Para este
caso se cuenta con ESCOW14, un corpus en español de más de siete billones de palabras repartidas en
cinco millones de documentos. El corpus fue generado realizando crawling masivo en Internet y en su
versión actual cuenta con documentos del período comprendido entre 2012 y 2014, abarcando además
documentos de una gran cantidad de países de habla hispana.

Para la limpieza de documentos obtenidos de la web y posterior extracción de texto útil se utilizó
texrex, una herramienta creada por Roland Schäfer, integrante del equipo de COW. La herramienta cuenta
con funcionalidades para filtrar duplicados, remover código HTML, extraer metadatos de los cabezales
HTTP obtenidos del proceso de crawling, normalización de codificación de caracteres a UTF-8, entre
otras. Este proceso resulta de mucha importancia pues en su descripción del corpus los autores
mencionan como una de sus prioridades generar corpus de alta calidad, y no solamente enfocarse en el
tamaño de los mismos.

El acceso a ESCOW14 está altamente limitado por los autores y regido por políticas de acceso muy
estrictas, en un esfuerzo por ajustarse a las leyes alemanas de derecho de autor. Por este motivo
los autores ofrecen para descargar solamente grupos de oraciones mezcladas lejos de su contexto inicial
y no permiten acceso directo a los documentos originales. Además, para obtener acceso a este contenido
es necesario completar un formulario detallando el uso que se planea hacer del corpus, entre otros
datos. Lamentablemente, al momento de escribir este informe aún no fuimos capaces de obtener acceso
a ESCOW14.

\subsection{Conclusiones}

La investigación realizada demuestra la dificultad que existe actualmente para acceder a un corpus en
español de gran tamaño. Se comprobó que la mayoría de los proyectos existentes colocan
numerosas restricciones para el acceso a sus recursos. En muchos casos la restricción es económica,
y se requiere pagar para acceder a los textos. En otros, los corpus son ofrecidos de forma gratuita pero
se limita la cantidad de consultas que pueden realizarse sobre los mismos. Es también común que se
permita el acceso a los datos solamente a través de interfaces gráficas, útiles en ciertos contextos, pero
que no sirven para nuestro caso de estudio.

Se llegó pues a la conclusión de que para alcanzar el objetivo inicial de un corpus con dos mil millones
de palabras, resultaría necesario construirlo de forma independiente. En las próximas secciones se describe
el proceso llevado a cabo para cumplir este objetivo.

\section{Proceso de Construcción}

En esta sección se presentan las características buscadas del corpus a generar y el proceso seguido
para lograrlo.

\subsection{Requerimientos}

Previo al inicio de la construcción del corpus, se establecieron una serie de requerimientos que el
mismo debiera cumplir:

\begin{itemize}

\item Puesto que el tamaño del corpus afecta directamente la calidad de las representaciones
vectoriales, es claro que el principal requerimiento es que el resultado tenga una gran cantidad de
palabras. El mínimo que se planteó al inicio del proyecto fue de dos mil millones de palabras (para
comparar, el corpus en inglés utilizado por Mikolov en~\cite{Mikolov2013c} ronda los seis mil
millones de palabras).

\item La única restricción que se plantea sobre el contenido es que sea texto en forma libre (sin
ningún tipo de estructura a priori), y que esté limpio: esto es, que no cuente con \textit{markup}
de ningún tipo. Dado que los algoritmos para construir los vectores trabajan con una secuencia de
palabras sin formato alguno, es deseable que el texto se procese incluso antes de ser almacenado,
para evitar tener que tratarlo más adelante.

\item Se desea que el corpus esté dividido en documentos individuales que sean coherentes en sí
mismo, manteniendo un determinado estilo de escritura internamente. Así, artículos de noticias serán
almacenados individualmente, y lo mismo con libros, artículos de Wikipedia, y cualquier otro texto
que se recopile. El objetivo de este punto es permitir excluir o borrar un documento particular en
caso de ser necesario.

\item Se espera también que estos documentos puedan tener ciertos metadatos asociados. Algunos de
estos datos pueden aplicar a todos los documentos, como la fecha en la que fue agregado al corpus,
mientras que otros sólo tendrán sentido para un subconjunto, como la fecha de publicación de una
noticia. Esto puede permitir eventualmente construir un corpus con las noticias de un año
particular, para poder generar representaciones vectoriales más finas.

\item Es de suma importancia tener trazabilidad sobre los distintos documentos del corpus para
garantizar su calidad: esto es, saber de qué sitio se extrajo el texto de cada documento, en qué
fecha, y mediante qué proceso. Así, en caso de haber un problema en el proceso de extracción, es
siempre posible revertir el corpus a un estado anterior, eliminando todos los documentos
erróneos. Este punto permite además evitar almacenar más de una vez un documento particular, y ayuda
también a identificar cuáles contenidos están protegidos por \textit{copyright}.

\item Con el fin de generar buenas representaciones vectoriales, es importante que el corpus cuente
con variedad de texto: texto que tenga distintos estilos de escritura, que utilicen vocabularios
distintos, y que tengan distintos niveles de correctitud ortográfica. Esto dota a los algoritmos de
un mayor vocabulario y de información más rica acerca del contexto en el que las palabras pueden
aparecer.

\item Por último, puesto que el corpus debe contar con gran variedad, es necesario que esté
ordenado. Se espera que cada documento esté etiquetado en base a sus cualidades, identificando
cuáles son artículos de noticias, cuáles son de determinado país, etc. Etiquetar el texto brinda la
posibilidad de construir un corpus a medida, para poder eventualmente hacer pruebas específicas que
incluyan o exculyan cierto tipo de texto.

\end{itemize}

Los requerimientos recién planteados son exigentes, pero permiten la construcción de un corpus
altamente versátil. Además de poder entrenar modelos vectoriales particulares a una aplicación (en
base al texto que se incluya en el corpus), permite que el mismo pueda ser utilizado en otras
investigaciones que no estén relacionadas a las representaciones vectoriales, pero que necesiten de
texto en español con ciertas cualidades. Mediante la inclusión de metadatos particulares, de
etiquetado según el estilo del texto, y de datos de trazabilidad para cada documento, se busca
reforzar esta idea.

El hecho de exigir tantos requerimientos, sin embargo, hace que sea necesario descartar algunos de
los corpus existentes anteriormente mencionados. [o no? Cuáles de los corpus existentes nos sirven y
cuáles no (si están limpios, clasificados, etc.)?]


\subsection{Relevamiento de Fuentes de Palabras}

Se comenzó por la tarea de evaluar de dónde obtener texto libre de Internet, con el fin de construir
un corpus de calidad que cumpla con los requerimientos anteriores. En esta sección enumeraremos los
distintos tipos de sitios de donde se recopiló el texto, junto a las características del mismo y una
breve descripción de cómo se realizó la extracción. En la siguiente sección se entrará en la
implementación de estas técnicas en más detalle.

[- comentar el tipo de texto que se saca de cada fuente]
[- comentar cómo se saca el texto limpio para cada fuente comentada]
[el objetivo de esta sección es mostrar que buscamos *muchas* fuentes de datos y nos quedamos con lo
mejor; que cada una de las fuentes cumplió con los requerimientos y más o menos qué enfoque se
utilizó para scrapearla]

[- comentar que se optó por sacar algo de cada tipo de fuentes, que en principio hay más info en cada área pero escapaba del alcance]


\subsubsection{Portales de Noticias}

La primer fuente de palabras investigada fueron los portales de noticias en línea, pues presentan
varias ventajas:\begin{inparaenum}[(a)]
\item están compuestos de texto por lo general bien escrito;
\item están divididos en documentos que tratan cada uno de un tema particular;
\item cada portal tiene un gran número de artículos y hay una gran cantidad de portales por país;
\item permiten obtener muestras de texto por país, mejorando la riqueza del corpus final; y
\item el texto es fácil de limpiar, pues el único markup con el que cuentan suele ser HTML\@.
\end{inparaenum}

En cuanto al vocabulario utilizado en los artículos de noticias, se puede considerar simple, pero
con una fuerte presencia de nombres de personas y lugares, que a su vez son dependientes de la fecha
de publicación del artículo: esto es, distintas personas tendrán más o menos menciones en períodos
particulares de tiempo. Este punto es importante, y es una de las principales razones para decidir
guardar esta fecha de como metadato del artículo.


La investigación de portales de noticias comenzó por el diario La Nación de Argentina [cita?], por
ser uno de los primeros diarios grandes en tener presencia digital, desde el año 2000. Realizando
algunos cálculos preliminares en base a un muestreo de artículos, se vio que el diario contaba con
al menos un millón de artículos, cada uno con un promedio de 500 palabras, llegando a un a masa total
de palabras muy importante.

Uno de los primeros puntos que se notó es que la URL de todos los artículos del portal estaban
identificadas por un número, y que cambiando este número por uno distinto, se podía acceder a otra
noticia. Además, la generación de estos números es secuencia: esto es, el primer artículo publicado
tiene el identificador $1$, mientras que el último el $1871018$. Este hallazgo fue de suma
importancia, pues permitió realizar \textit{scraping} del portal de forma más directa.

Aprovechando esta característica, se construyó un script en Python [cita?] para recorrer la lista de
identificadores (desde el $1$ hasta el último presente en la portada) y descargar el contenido de
los artículos. Puesto que el corpus a construir requiere de texto limpio, se generaronn una serie de
reglas XPath [cita?] para obtener los datos estructurados (título de la noticia, contenido, fecha de
publicación). Corriendo este script en la totalidad de La Nación, se llegó a la cifra de 600
millones de palabras exclusivamente con sus artículos.

Los resultados obtenidos demostraron el alto potencial que tienen los portales de noticias como
fuentes de palabras, por lo que se procedió a recopilar una lista de los principales diarios de
América Latina y España. Con el objetivo de seguir el esquema anterior, se identificaron cuáles de
estos sitios identificaban sus artículos con las mismas características, y se adaptó el anterior
script a los 20 que presentaban un mayor potencial (en base a la actividad y al contenido).

Para cada uno de estos portales, se construyeron una serie de reglas XPath para extraer el contenido
y un mecanismo para recorrer por el identificador numérico. Además se adaptó el script para que
revise de forma continua por noticias nuevas, con el fin de que el corpus se mantenga siempre
actualizado\footnote{Para ilustrar la importancia de este punto, sólo por mantener actualizado el
corpus se consiguieron 300 millones de palabras adicionales en el tiempo de proyecto
transcurrido.}. Este método lo denominamos \textit{scraping automático}, y más adelante se detallará
su implementación (por ejemplo, cómo sabe extraer los datos, cómo se define un nuevo sitio, y cómo se
hace la recorrida por identificador).

En base a esta técnica se consiguieron más de 3 mil millones de palabras de texto limpio a partir de
20 portales de noticias.


Cabe resaltar que el método anterior depende de que los artículos de noticias tengan asociado un
identificador secuencial. Lamentablemente, esto no es el caso con muchos sitios de noticias
importantes y de gran tamaño, como El País de Madrid, El País de Uruguay, o Clarín de Argentina. Por
esta razón, se decidió también emplear un mecanismo de scraping estándar mediante la herramienta
Scrapy [cita?] para estos portales, implementación que se detalla más adelante.


\subsubsection{Escritura Amateur}

Existen diversos sitios en Internet dedicados a la escritura amateur, donde los usuarios se
registran y publican historias propias. Puesto que muchos de estos sitios acumulan una gran cantidad
de historias, se decidió investigar esta área en mayor profundidad.

Algunos de estos sitios, como Los Cuentos [cita], son de carácter más bien general, donde los
usuarios publican narraciones, ensayos, cuentos o poemas. La calidad y el contenido de estos textos
varía drásticamente entre uno y otro, pudiendo encontrar piezas con una gran cantidad de faltas de
ortografía y carentes de sentido, o piezas por autores renombrados que fueron republicados en el
sitio.

La otra gran categoría de escritura amateur presente en Internet es el \textit{fanfiction}
(\textit{ficción de fans}, en inglés). Estos textos suelen ser escritos principalmente por jóvenes,
centrados en universos o personajes de la cultura popular, como series de televisión, videojuegos o
libros. Debido a esto, su calidad suele ser incluso inferior al anterior grupo. Suelen contener
grandes cantidades de diálogo (como si fuera un guión) y muchas menciones a personajes o lugares
ficticios.

De todos modos, la media de los textos obtenidos de estas fuentes de palabras está compuesta por
oraciones coherentes y con relativamente pocas faltas de ortografía, por lo que es de gran utilidad
para el corpus que se pretende construir. Además, existe un gran volumen de datos, y el texto es
fácil de limpiar.

Para atacar estos sitios, se optó por usar principalmente la técnica de scraping automático
mencionada en la sección anterior, pudiendo así incorporar al corpus nuevas historias en la medida
que se escriban.


\subsubsection{Wikimedia}

Los distintos proyectos de Wikimedia [cita], como Wikipedia, Wikilibros, Wikicitas, y Wikiviajes,
son una fuente de palabras particularmente interesante, pues tiene todas las cualidades que se
espera del corpus:

\begin{itemize}

\item En primer lugar, cuentan con grandes volúmenes de texto que ayudan a llegar a la meta
propuesta. El principal proyecto de Wikimedia es Wikipedia, pero el resto tienen cantidades no
despreciables de palabras también.

\item Segundo, tienen una gran variedad de contenidos, en especial Wikipedia: siendo una
enciclopedia, posee entradas y menciones para países, personas, lugares, eventos, fenómenos, etc.,
dotando así al corpus de un gran y diverso vocabulario, lo cuál será muy positivo al entrenar
modelos vectoriales.

\item Tercero, el texto está bien escrito y suele ser activamente revisado por distintas personas,
por lo que el nivel de calidad es superior al resto de las fuentes.

\item Cuarto, se encuentra dividido en artículos bien delimitados que tratan de un único tema
particular, permitiendo obtener resultados más coherentes cuando se realizan búsquedas sobre el
corpus.

\end{itemize}


El contenido de los proyectos de Wikimedia puede ser descargado en línea a través de \textit{dumps}
provistos por la fundación misma [cita]. La principal complejidad de incorporar esta fuente es, sin
embargo, su limpieza, pues los artículos vienen con un \textit{markup} denominado \textit{wikitexto}
[cita] que dificulta la extracción del texto. De todos modos, existen una gran variedad de
herramientas para la limpieza del mismo, cuestión que se detallará más adelante.


\subsubsection{Foros}

Los foros de discusión de Internet [cita] son otra gran fuente de palabras, en cuanto al volumen de
datos. Dado que muchos poseen miles de usuarios registrados, llegan a manejar cantidades de texto
muy elevadas. Como contrapartida, sin embargo, el texto publicado suele ser de muy baja calidad,
donde algunos mensajes no son más que imágenes o emotíconos y por lo tanto carentes de contenido
útil para el propósito del proyecto. Esto último depende también de la audiencia que interactúe en
dicho foro, donde sitios con una demografía de edad mayor suelen tener mejor calidad.

De todos modos, se decidió utilizar estas plataformas como fuentes de noticias porque permiten
experimentar con la calidad de los modelos vectoriales en función a la calidad del corpus. Además,
puesto que suelen tener un estilo más similar al habla, tienen vocabularios distintos al resto de
las fuentes y consecuentemente una mayor cantidad de expresiones del argot de los participantes.

Se utilizaron técnicas de scraping a través de Scrapy para extraer el contenido de estas fuentes,
punto que se detallará en la sección siguiente.


\subsubsection{Subtítulos}

Hay una gran cantidad de texto en los subtítulos de series y películas extranjeras. Como referencia,
los subtítulos de un capítulo de una serie de televisión de una hora rondan las cinco mil
palabras. Dada la enorme cantidad de series existentes, cada una con decenas de episodios, se llega
a un volumen de palabras muy alto.

El texto de los subtítulos por lo general está correctamente escrito, aunque se compone casi
exclusivamente de diálogos. Esto también provee otro tipo de vocabulario distinto a las fuentes
anteriores pues, por ejemplo, hay un mayor uso de la segunda persona.

La descarga de los subtítulos se realizó con un script manual, bajando todo el contenido disponible
en Tu Subtítulo [cita]. Luego fue necesario extraer el texto de los mismos, eliminando las
indicaciones de tiempo (cuándo debe aparecer cada línea en pantalla) y el markup que pueden contener
(tags HTML por lo general).


\subsubsection{Documentos Oficiales}

Muchas instituciones públicas publican grandes cantidades de textos con el fin de informar a sus
poblaciones sobre distintos temas, o de mantener registro de las actividades de sus
representantes. Entre este tipo de texto se encuentran las versiones taquigráficas de sesiones de
parlamentos de países u organizaciones internacionales, como la UE o UNASUR, así como comunicados de
distintas instituciones como la ONU\@. Estos textos suelen tener una jerga partícular, pues son
muchas veces documentos altamente técnicos, pero son no obstante textos de calidad que pueden
aportar mayor volumen y vocabulario al corpus.

Se incorporaron dos fuentes de documentos oficiales al corpus: los diarios de sesiones del
parlamento Uruguayo del año 1985 a la fecha y los comunicados emitidos por la ONU entre los años
1999 a 2009.

El primero fue descargado a través de un script de la página del Poder Legislativo Uruguayo [cita] y
luego procesado para eliminar el markup. Contiene las sesiones de la Cámara de Diputados, la Cámara
de Senadores, y de la Asamblea General. El mecanismo para su descarga se detalla más adelante.

El segundo fue tomado de un corpus paralelo generado por [cita] como insumo para tareas de
traducción automática. Fue descargado de [cita http://opus.lingfil.uu.se/MultiUN.php], procesado y
finalmente agregado al corpus construido.


\subsubsection{Libros}

Una fuente de palabras inmediata son libros de texto en español, pues se cuenta con cientos de
miles, cada uno de gran tamaño. En la mayoría de los casos también son revisados por un editor, por
lo que la calidad de la escritura es superior a la encontrada en las otras fuentes. Además cuentan
con una gran variedad de vocabulario, pues su contenido va desde de ficciones narrativas hasta
textos científicos.

La incorporación de libros en el corpus presenta, sin embargo, varias dificultad: en primer lugar,
no es fácil acceder digitalmente a colecciones importantes de libros, pues no están disponibles de
manera pública en Internet fácilmente. Segundo, incluso cuando sí es posible obtener libros
digitalmente, suelen estar en formatos que hacen la extracción de texto muy compleja (por ejemplo,
PDFs con escaneos del texto).

Se optó por utilizar un corpus ya existente que recopila textos relacionados a la Unión Europea (en
particular a su librería, \textit{EU Bookshop} [cita]), provista por el proyecto \textit{LetsMT!}
[cita letsmt.eu]. Este es un corpus paralelo para la traducción automática, pero es posible utilizar
únicamente los textos en español para incorporar al corpus.

Está compuesto principalmente por publicaciones escritas por instituciones de la UE, por lo que
suelen ser de carácter más bien oficial (esto es, tratados, informes, y similares). Aún así, cuenta
con algunos libros de lectura general.


\subsubsection{Otros Corpus}

Se realizó también una investigación de los corpus en español existentes en la comunidad, como se
mencionó en el capítulo anterior. Algunos de ellos, como los corpus paralelos de traducción
automática a los que se hizo referencia arriba, fueron incluso incorporados al corpus.

[alguno más? ya se mencionaron algunos arriba pero en la sección correspondiente]


\section{Implementación}

A continuación se detalla la implementación del proceso de extracción del texto. Se comienza
describiendo la arquitectura de la solución, fundamentando las decisiones tomadas, y luego se
especifican las distintas técnicas empleadas para la obtención del texto.

[para cada una de las técnicas, a qué fuentes aplica]

\subsubsection{Infraestructura}

- se realiza una investigación de cuáles son las alternativas para almacenar grandes volúmenes de
  texto, evaluando tanto soluciones SQL como NoSQL, optando finalmente por Elasticsearch [cita?]

Elasticsearch [cita?] es un almacén de documentos JSON (similar a una base de datos NoSQL [cita?])
que provee funcionalidades de búsqueda \textit{full-text} (esto es, texto libre sin ningún tipo de
estructura) basado en la biblioteca de este propósito Lucene [cita?]. Provee una interfaz HTTP
RESTful [cita?] para el acceso, la consulta y el almacenamiento de los documentos con los que
trabaja.

Se optó utilizar Elasticsearch porque brinda varias ventajas por sobre las alternativas:

\begin{itemize}

\item Permite el almacenamiento grandes volúmenes de datos sin mayores problemas. Es capaz que
manejar millones de documentos con ajustes mínimos. [algomás]

\item Brinda funcionalidades de búsqueda de texto que, mientras que no son un requerimiento
\textit{a priori}, son de gran utilidad a la hora de explorar el corpus. [algomás] [qué implica
full-text search, búsqueda más inteligente (stemming, etc.)]

\item schemaless, por lo que no todos los documentos tienen que tener el mismo formato

\item permite mezclar datos estructurados y texto libre, pudiendo realizar consultas avanzadas, como
cantidad de palabras por fuente de datos, etc.

\item puede, en caso de ser necesario, adaptarse fácilmente a una infraestructura distribuida, con
replicación y balanceo de carga para poder manejar incluso mayor cantidad de documentos

\end{itemize}

Es importante mencionar que, comparado con un RDBMS más establecido como PostgreSQL, Elasticsearch
no carece de problemas. Es sabido que bajo ciertas circunstancias, es posible que ocurra una pérdida
de documentos [cita aphyr], aunque esto ocurre en esquemas distribuidos y bajo casos muy
particulares que no nada frecuentes. Además, el hecho de estar almacenando información no crítica
(esto es, no son registros de usuarios, por ejemplo), esto no plantea mayores riesgos.

---

- servidor en alemania con X características
- ahí corre elasticsearch, el código python, la herramienta web (después se detallará),
  postgresql (para el scraping automático y para la API (job management))

- backups en amazon S3 (funcionalidad integrada en elasticsearch)


\subsubsection{Scraping Automático}

Como se mencionó en la sección anterior, para el \textit{scraping} de los portales de noticias que
referencian a sus artículos con un identificador secuencial, se construyó un sistema especializado
para la tarea de extracción de los mismos.

Este sistema se compone de una base de datos con información de las fuentes de datos y los
artículos que son necesarios descargar; un módulo de scraping que se encarga de descargar, procesar
y almacenar en Elasticsearch cada uno de estos artículos; y un conjunto de módulos de extracción que
especifican, para cada sitio, cómo realizar el procesamiento de sus artículos. La arquitectura se
puede visualizar en el diagrama [diagrama].

A continuación detallaremos cada uno de estos componentes.


\paragraph{Modelo de Datos}

El modelo de datos consiste de fuentes de datos (\texttt{DataSource}) y de entradas
(\texttt{Entry}), almacenados en una base de datos \textit{PostgreSQL} [cita?]. Los
\texttt{DataSource} son un modelo simple que describe las fuentes de palabras registradas en el
sistema y cuenta básicamente del dominio del sitio (e.g. \texttt{lanacion.com.ar}), con el que se
identifica a la fuente. Además mantiene dos atributos adicionales: si está activo (esto es, si se
tienen que extraer documentos del mismo), y el nivel de concurrencia con el cuál realizar pedidos
(para no saturar a la fuente; su funcionamiento se detalla más adelante).

Por otro lado, las \texttt{Entry} son registros que identifican cada documento de cada fuente de
palabras, junto con el identificador a nivel de dicha fuente, el resultado del proceso de
extracción, la fecha en la que se agregó, el último intento, la cantidad de intentos, y la fuente a
la que pertenece. El resultado del proceso puede ser \textit{pendiente}, cuando todavía no se
intentó descargar el artículo asociado, \textit{éxito}, cuando se extrajo correctamente el
contenido, o uno de tres indicadores de error (\textit{no encontrado}, \textit{imposible de
procesar}, o \textit{intento fallido}, que se describen más adelante). Con la cantidad de intentos
se busca evitar que un artículo no se descargue por un error incidental, buscando reintentar hasta
cinco veces dicha acción.

Así, es posible mantener un registro detallado y estadísticas de todo el proceso de extracción. Para
cada fuente de datos, es posible saber cuántas entradas fueron revisadas, qué porcentaje de éstas no
contienen un documento asociado, y si alguna fuente está teniendo problemas en la extracción. Esto
último ayuda, por ejemplo, a detectar rápidamente un cambio de diseño en un sitio que provoque que
las reglas XPath queden obsoletas.


\paragraph{Módulos de Extracción}

Los módulos de extracción describen cómo es la interacción con los sitios de noticias en tres
aspectos: en cómo obtener la lista de identificadores que faltan procesar, en cómo obtener el
contenido de un artículo dado su código HTML, y en cómo obtener metadatos adicionales del artículo
(como su fecha de publicación). Para ello se define una interfaz que cada módulo debe implementar,
consistente de los siguientes elementos:

\begin{itemize}

\item Constantes \texttt{SOURCE\_DOMAIN} y \texttt{DOCUMENT\_URL}, que especifican el nombre de la
fuente (e.g. \textit{lanacion.com.ar}) y la URL de sus documentos dado un identificador,
respectivamente.

\item Función \texttt{get\_missing\_ids}, que recibe una lista de identificadores ya existentes en la
base de datos y devuelve los identificadores faltantes. Para esto la función puede, por ejemplo,
entrar a la página inicial del portal y obtener el identificador asociado a la noticia más reciente.

\item Función \texttt{get\_content}, que recibe la respuesta del portal ante el pedido de un artículo
particular. Esta función deberá revisar el código de respuesta para asegurarse que efectivamente se
haya encontrado un artículo. Aquí se encapsula la lógica de extracción del texto limpio, ya sea
mediante la utilización de reglas XPath o selectores CSS, mediante la ayuda de la biblioteca
\texttt{lxml} [cita?]. En caso de tener éxito, devolverá el contenido del artículo y un valor de
salida \texttt{success}; en caso de haber un error, devolverá \texttt{notfound}, \texttt{failure}, o
\texttt{unparseable}. El objetivo de esta distinción es saber si es necesario reintentar porque el
fallo fue por un error circunstancial (caso \texttt{failure}), si no existe un artículo (caso
\texttt{notfound}), o si hubo un problema al extraer los datos (caso \texttt{unparseable}), en cuyo
caso será necesario revisar el proceso de extracción.

\item Función \texttt{get\_metadata}, que recibe la respuesta de un artículo para el cual se extrajo
exitosamente el contenido, y devuelve los metadatos pertinentes al documento (fecha de publicación,
título, etc.).

\end{itemize}

Así, es posible incorporar una nueva fuente de palabras simplemente agregando un archivo Python que
siga la anterior interfaz. El módulo de scraping se encargará así de dar de alta la nueva fuente en
la base de datos y empezar a descargar el contenido. Esto brinda un diseño flexible y fácil de
extender en un futuro [más énfasis en esto?].


\paragraph{Módulo de Scraping}

El sistema corre como un \textit{daemon} que realiza el siguiente ciclo:

\begin{itemize}

\item Revisa cuáles son las fuentes de datos existentes, junto a su configuración, dando de alta en
la base de datos cualquier fuente que no tenga un registro, en base a los módulos existentes.

\item Para cada fuente de datos, crea las entradas faltantes en la tabla correspondiente, en base a
lo que le indique el módulo de extracción asociado a la fuente.

\item Para cada entrada cuyo estado es \textit{pendiente}, se descarga el HTML del artículo asociado
y se extrae tanto el contenido como los metadatos del mismo, de acuerdo a lo especificado en el
módulo de extracción asociado a la fuente.

\item Se actualiza la entrada con el resultado obtenido, y se almacena el documento extraído en
Elasticsearch con el formato adecuado.

\item Cuando no quedan más entradas pendientes, duerme por 15 minutos y vuelve a empezar el ciclo.

\end{itemize}

Con el fin de no tener que esperar por las respuestas de los sitios de noticias uno a uno, lo que
implicaría una espera de décimas de segundos para cada documento que haría imposible la descarga de
los millones que se requieren, el módulo de scraping se implementa como un bucle de eventos.

Un bucle de eventos (\textit{event loop}, en inglés), es una construcción que permite ejecutar
concurrentemente funciones que dependen de eventos externos bloqueantes (como acceso a un
dispositivo de Entrada/Salida) sin que el hilo del programa principal tenga que bloquearse
esperando, de manera similar a como hace un sistema operativo de tiempo compartido.

Ésta técnica consiste de un bucle principal que se encarga de ejecutar una serie de corrutinas, las
cuales se suspenderán cuando sean bloqueadas esperando un evento externo y continuarán su ejecución
cuando el bucle principal se lo indique. Para esto, el bucle debe emplear técnicas de
\textit{polling} (o la funcionalidad equivalente que provea el sistema operativo) para saber cuándo
un evento está listo para ser atendido. [figura? cita?]

Se utilizó la implementación de la biblioteca estándar de Python de esta construcción, denominada
\texttt{asyncio} [cita?]. De esta forma, es posible descargar cientos de artículos concurrentemente,
sin demorarse por un sitio particularmente lento. Para ello, cuando se recorren las entradas
pendientes y se realiza un pedido HTTP para obtener el artículo, la función que procesa los
artículos se suspende hasta que la descarga finalice, continuando con otro artículo mientras tanto.

De todos modos, es necesario limitar la cantidad máxima de concurrencia por dos razones: primero,
porque se corre el riesgo de iniciar miles o hasta millones de pedidos en simultáneo, lo cuál es
computacionalmente muy costoso e improductivo; y segundo, porque no se quiere saturar a los sitios
de noticias con demasiados pedidos activos, ya que se correría el riesgo de que se le prohiba el
acceso a nuestro sistema. Algunos sitios son particularmente susceptibles a este último punto, por
lo que se decidió que el nivel de concurrencia sea configurable a nivel de cada fuente de datos.


Para concluir esta sección, presentamos algunos números del proceso. Mediante este sistema se
extrajeron [X] documentos de 20 portales de noticias, obteniendo así un total de [Y] mil millones de
palabras. Se dieron de alta un total de [W] entradas, de las cuales un $[Z]\%$ llevó a la extracción
exitosa de un documento, donde algunas fuentes de noticias fueron más rentables que otras, como se
puede ver en [figura: tabla de porcentajes].


\subsubsection{Scraping Manual}


--- Con Scrapy

Para las fuentes de palabras que carecen de una estructura que favorece su extracción automática
mediante el método anterior, se utilizaron también técnicas de scraping convencionales, a través de
\textit{Scrapy} [cita?].

- qué es scrapy
- qué diferencias tiene con lo anterior (y similitudes), y para qué casos es útil
    - ejemplo: en El País Uruguay es necesario primero entrar al archivo, luego a la portada
      histórica y finalmente al artículo particular
- en qué fuentes de datos se usa: el país madrid/uruguay, clarín; fanfics? foros?
- cómo se corre (una vez sola, aunque se puede volver a correr y actualizar los documentos)


--- Scripts de limpieza

- para casos como wikipedia, se puede bajar el contenido directamente a través de un dump, pero está
  sucio (markup)
- se utiliza un script de limpieza y se indexa directamente en Elasticsearch, con las etiquetas
  adecuadas (decir cuál script)

- lo mismo para la página del parlamento (si lo terminamos metiendo)
- lo mismo para los corpus ya existentes (si metemos alguno)


\section{Resultado}

En esta sección se pretende presentar un análisis de la composición y la calidad del corpus
construido, mostrando algunas cifras por vertical de texto.


- resultado del corpus obtenido en total de palabras (resaltar que el obj eran 2B)
- analizar por tags (noticias, wikipedia, países, fechas, etc.)
- análisis de calidad/composición del corpus
    - palabras más comunes, etc. (algún dato sobre el corpus más general)
    - duplicados/texto sucio (muestreo de documentos aleatorios?)
    [tweets embebidos en noticias o algo de eso]


----

[hablar algo sobre temas legales?]
[en el resto del cap ir contando cómo se satisface cada requerimiento]

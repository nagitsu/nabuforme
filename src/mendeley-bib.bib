@article{AlRfou2013,
abstract = {Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.},
archivePrefix = {arXiv},
arxivId = {1307.1662},
author = {Al-Rfou, Rami and Perozzi, Bryan and Skiena, Steven},
eprint = {1307.1662},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731.pdf:pdf},
journal = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
pages = {183--192},
title = {{Polyglot: Distributed Word Representations for Multilingual NLP}},
year = {2013}
}
@article{Almuhareb2006,
author = {Almuhareb, Abdulrahman},
journal = {PhD Thesis, University of Essex},
keywords = {Concepts,Distributional semantics},
title = {{Attributes in Lexical Acquisition}},
year = {2006}
}
@article{Wacky2009,
abstract = {This article introduces ukWaC, deWaC and itWaC, three very large corpora of English, German, and Italian built by web crawling, and describes the methodology and tools used in their construction. The corpora contain more than a billion words each, and are thus among the largest resources for the respective languages. The paper also provides an evaluation of their suitability for linguistic research, focusing on ukWaC and itWaC. A comparison in terms of lexical coverage with existing resources for the languages of interest produces encouraging results. Qualitative evaluation of ukWaC vs. the British National Corpus was also conducted, so as to highlight differences in corpus composition (text types and subject matters). The article concludes with practical information about format and availability of corpora and tools},
author = {Baroni, Marco and Bernardini, Silvia and Ferraresi, Adriano and Zanchetta, Eros},
doi = {10.1007/s10579-009-9081-4},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/3b9ba3d40d8f1b4efd53dd81439d3a5dd4ee0629.pdf:pdf},
isbn = {1057900990},
issn = {1574020X},
journal = {Language Resources and Evaluation},
keywords = {Annotated corpora,Corpus construction,English,General-purpose linguistic resources,German,Italian,WaCky!,Web as corpus},
number = {3},
pages = {209--226},
title = {{The waCky wide web: A collection of very large linguistically processed web-crawled corpora}},
volume = {43},
year = {2009}
}
@article{Baroni2014,
abstract = {Context-predicting models (more com- monly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the litera- ture is still lacking a systematic compari- son of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counter- parts.},
author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, German},
doi = {10.3115/v1/P14-1023},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/a9036b159f33f867901410174335a18c54aca5fd.pdf:pdf},
isbn = {9781937284725},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.},
pages = {238--247},
title = {{Don't count, predict! A systematic comparison of context-counting vs . context-predicting semantic vectors}},
year = {2014}
}
@article{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{Berardi2015,
author = {Berardi, Giacomo and Esuli, Andrea and Marcheggiani, Diego},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/c38a66bd7f71855e2e002331b55578c4c3606734.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Glove,Word embeddings,Word2vec},
title = {{Word embeddings go to Italy: A comparison of models and training datasets}},
volume = {1404},
year = {2015}
}
@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/45aa6b6143c3fe6fa7abf0127a273656e844d855.pdf:pdf},
isbn = {9781577352815},
journal = {Journal of Machine Learning Research},
keywords = {lda,topic model},
number = {4-5},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2012}
}
@article{Brown1992,
abstract = {We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.},
author = {Brown, Peter and DeSouza, Peter and Mercer, Robert and Pietra, Vincent and Lai, Jenifer},
journal = {Comput. Linguist.},
number = {4},
pages = {467--479},
title = {{Class-based n-gram models of natural language}},
volume = {18},
year = {1992}
}
@article{Bruni2012,
author = {Bruni, Elia and Gatica-perez, Daniel},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/a222bb311e8da7a8c86cbb8d169305995437342b.pdf:pdf},
number = {December},
title = {{Multimodal distributional semantics Marco Baroni , Thesis Advisor}},
volume = {48},
year = {2013}
}
@article{BullinariaLevy2007,
abstract = {The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures.},
author = {Bullinaria, John a and Levy, Joseph P},
issn = {1554-351X},
journal = {Behavior research methods},
keywords = {Data Interpretation,Humans,Models,Psychological,Semantics,Statistical,Vocabulary},
number = {3},
pages = {510--26},
pmid = {17958162},
title = {{Extracting semantic representations from word co-occurrence statistics: a computational study.}},
volume = {39},
year = {2007}
}
@article{BullinariaLevy2012,
abstract = {In a previous article, we presented a systematic computational study of the extraction of semantic representations from the word-word co-occurrence statistics of large text corpora. The conclusion was that semantic vectors of pointwise mutual information values from very small co-occurrence windows, together with a cosine distance measure, consistently resulted in the best representations across a range of psychologically relevant semantic tasks. This article extends that study by investigating the use of three further factors--namely, the application of stop-lists, word stemming, and dimensionality reduction using singular value decomposition (SVD)--that have been used to provide improved performance elsewhere. It also introduces an additional semantic task and explores the advantages of using a much larger corpus. This leads to the discovery and analysis of improved SVD-based methods for generating semantic representations (that provide new state-of-the-art performance on a standard TOEFL task) and the identification and discussion of problems and misleading results that can arise without a full systematic study.},
author = {Bullinaria, John a and Levy, Joseph P},
doi = {10.3758/s13428-011-0183-8},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/715115f21d206ac35c488d775fe30b14b262c327.pdf:pdf},
journal = {Behavior research methods},
keywords = {Artificial Intelligence,Choice Behavior,Data Interpretation,Humans,Judgment,Psycholinguistics,Semantics,Software,Statistical},
number = {3},
pages = {890--907},
pmid = {22258891},
title = {{Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD.}},
volume = {44},
year = {2012}
}
@article{Caron2001,
author = {Caron, John},
isbn = {0-89871-500-8},
journal = {In Proceedings of SIAM Computational Information Retrieval Workshop},
keywords = {Distributional semantics},
pages = {1--14},
title = {{Experiments with LSA scoring: Optimal rank and basis}},
year = {2000}
}
@article{ChurchHanks1990,
abstract = {The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words. 1 MEANING AND ASSOCIATION It is common practice in linguistics to classify words not only on the basis of their meanings but also on the basis of their co-occurrence with other words. Running through the whole Firthian tradition, for example, is the theme that "You shall know a word by the company it keeps" (Firth, 1957). On the one hand, bank co-occurs with words and expres-sion such as money, notes, loan, account, investment, clerk, official, manager, robbery, vaults, working in a, its actions, First National, of England, and so forth. On the other hand, we find bank co-occurring with river, swim, boat, east (and of course West and South, which have acquired special meanings of their own), on top of the, and of the Rhine. (Hanks 1987, p. 127) The search for increasingly delicate word classes is not new. In lexicography, for example, it goes back at least to the "verb patterns" described in Hornby's Advanced Learner's Dictionary (first edition 1948). What is new is that facili-ties for the computational storage and analysis of large bodies of natural language have developed significantly in recent years, so that it is now becoming possible to test and apply informal assertions of this kind in a more rigorous way, and to see what company our words do keep. 2 PRACTICAL APPLICATIONS},
author = {Church, Kenneth Ward and Hanks, Patrick},
doi = {10.3115/981623.981633},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/b4130e315df099e981342542536edfe36f77a20f.pdf:pdf},
journal = {Proceedings of the 27th Annual Conference of the Association for Computational Linguistics},
number = {1},
pages = {22--29},
title = {{Word association noms, Mutual Information, and lexicography}},
volume = {16},
year = {1989}
}
@article{CollobertWeston2008,
abstract = {We describe a single convolutional neural net- work architecture that, given a sentence, out- puts a host of language processing predic- tions: part-of-speech tags, chunks, named en- tity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semanti- cally) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data ex- cept the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the general- ization of the shared tasks, resulting in state- of-the-art performance.},
author = {Collobert, Ronan and Weston, Jason},
doi = {10.1145/1390156.1390177},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/ebe978c21cbc2a9e738b9fc9d257bbab2e093177.pdf:pdf},
isbn = {9781605582054},
issn = {07224028},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
number = {1},
pages = {160--167},
title = {{A unified architecture for natural language processing}},
volume = {20},
year = {2008}
}
@article{CollobertWeston2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling, achieving or exceeding state-of-the-art performance in each on four benchmark tasks. Our goal was to design a flexible architecture that can learn representations useful for the tasks, thus avoiding excessive task-specific feature engineering (and therefore disregarding a lot of prior knowledge). Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabelled training data. This work is then used as a basis for building a freely available tagging system with excellent performance while requiring minimal computational resources.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1145/2347736.2347755},
eprint = {1103.0398},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/37477d52e19671b497f68424431c794c2e3d13a1.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Deep Learning,Natural Language Processing,Neural Networks},
pages = {1--48},
title = {{Natural Language Processing (almost) from Scratch}},
volume = {1},
year = {2011}
}
@article{Deerwester1990,
abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.},
author = {Deerwester, Scott and Dumais, Susan T. and Harshman, Richard},
doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/fdf764ed05df7c8e83ddb9c5b58272b3bf59cf46.pdf:pdf},
issn = {0002-8231},
journal = {Journal of the American society for information science},
number = {6},
pages = {391--407},
title = {{Indexing by latent semantic analysis}},
volume = {41},
year = {1990}
}
@article{Dinu2013,
author = {Dinu, Georgiana},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/69a7244d3ae225451372d2709872049c4bf33c62.pdf:pdf},
journal = {Acl (2)},
number = {2010},
pages = {31--36},
title = {{DISSECT-DIStributional SEmantics Composition Toolkit}},
year = {2013}
}
@article{Dumais1988,
abstract = {The paper describes a new approach for dealing with the vocabulary problem in human-computer interaction. Most approaches to retrieving textual materials depend on a lexical match between words in users' requests and those in or assigned to database objects. Because of the tremendous diversity in the words people use to describe the same object, lexical matching methods are necessarily incomplete and imprecise. The latent semantic indexing approach tries to overcome these problems by automatically organizing text objects into a semantic structure more appropriate for matching user requests. This is done by taking advantage of implicit higher-order structure in the association of terms with text objects. The particular technique used is singular-value decomposition, in which a large term by text-object matrix is decomposed into a set of about 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination. Terms and objects aare represented by 50 to 150 dimensional vectors and matched against user queries in this "semantic" space. Initial tests find this completely automatic method widely applicable and a promising way to improve user's access to many kinds of textual materials, or to objects and services for which textual descriptions are available.},
author = {Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Deerwester, Scott and Harshman, Richard},
doi = {10.1145/57167.57214},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/a82a74f701c94d1f72b4c0386685d828f5a2dd53.pdf:pdf},
isbn = {0-201-14237-6},
journal = {ACM Conference on Human Factors in Computing Systems, CHI '88},
pages = {281--285},
title = {{Using Latent Semantic Analysis to Improve Access to Textual Information}},
year = {1988}
}
@article{Elman1990,
abstract = {[PDF]},
author = {Elman, J L},
doi = {10.1207/s15516709cog1402_1},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/69bfc8b2a1111acadd1d03d8994ce696795fc389.pdf:pdf},
issn = {03640213},
journal = {Cognitive science},
number = {2},
pages = {179--211},
pmid = {19563812},
title = {{Finding structure in time}},
volume = {14},
year = {1990}
}
@article{Finkelstein2002,
abstract = {We describe a new paradigm for performing search in context. In the IntelliZap system we developed, search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (“the context”). The involves context-guided semantic keyword extraction and clustering information retrieval process to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. The results are then semantically reranked, again, using context. It is our belief that letting context guide the search provides a better match to the user's current needs than just relying on the user's fixed personal profile. Our guide search effectively offers even inexperienced users an advanced search tool on the Web},
author = {Finkelstein, Lev and Gabrilovich, Evgeniy and Matias, Yossi and Rivlin, Ehud and Solan, Zach and Wolfman, Gadi and Ruppin, Eytan},
doi = {10.1145/503104.503110},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/722173a9a0c992812d3bee3ad3096c8841ad954d.pdf:pdf},
isbn = {1581133480},
issn = {10468188},
journal = {ACM Transactions on Information Systems},
keywords = {-based search engines are,a considerable amount of,although such systems seem,as a popular means,conceptual paradigm for performing,deceptively simple,for web-based,in widespread use today,information needs,information retrieval,largely automates the search,new,order to satisfy non-trivial,process,search in context that,skill is required in,this paper presents a},
number = {1},
pages = {116--131},
title = {{Placing search in context: the concept revisited}},
volume = {20},
year = {2002}
}
@article{Firth1957,
author = {Firth, J. R.},
title = {{Papers in Linguistics}},
year = {1957}
}
@misc{Reddit2016,
author = {Grefenstette, Edward},
title = {{AMA: Nando de Freitas}},
}
@article{Gutmann2012,
abstract = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, themodel is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective func- tion for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gener- ated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to be- have like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimationmethods for unnormalizedmodels. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.},
author = {Gutmann, Michael U},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1df176123ada3c3aebba3d9fcec55386091b0d13.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {computation,estimation,natural image statistics,partition function,unnormalized models},
pages = {307--361},
title = {{Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics}},
volume = {13},
year = {2012}
}
@incollection{Harris1954,
address = {Dordrecht},
author = {Harris, Zellig S.},
booktitle = {Papers on Syntax},
doi = {10.1007/978-94-009-8467-7_1},
pages = {3--22},
publisher = {Springer Netherlands},
title = {{Distributional Structure}},
year = {1954}
}
@article{Hill2014,
abstract = {We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, in contrast to gold standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness, so that pairs of entities that are associated but not actually similar [Freud, psychology] have a low rating. We show that, via this focus on similarity, SimLex-999 incentivizes the development of models with a different, and arguably wider range of applications than those which reflect conceptual association. Second, SimLex-999 contains a range of concrete and abstract adjective, noun and verb pairs, together with an independent rating of concreteness and (free) association strength for each pair. This diversity enables fine-grained analyses of the performance of models on concepts of different types, and consequently greater insight into how architectures can be improved. Further, unlike existing gold standard evaluations, for which automatic approaches have reached or surpassed the inter-annotator agreement ceiling, state-of-the-art models perform well below this ceiling on SimLex-999. There is therefore plenty of scope for SimLex-999 to quantify future improvements to distributional semantic models, guiding the development of the next generation of representation-learning architectures.},
archivePrefix = {arXiv},
arxivId = {1408.3456},
author = {Hill, Felix and Reichart, Roi and Korhonen, Anna},
doi = {10.1162/COLI},
eprint = {1408.3456},
isbn = {9781608459858},
issn = {04194217},
journal = {Computational Linguistics},
number = {4},
pages = {665--695},
title = {{SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation}},
volume = {41},
year = {2015}
}
@misc{Hinton1986,
abstract = {Concepts can be represented by distributed patterns of activity in networks of neuron-like units. One advantage of this kind of representation is that it leads to automatic generalization. When the weighjts in the network are changed to incorporate new knowledgwe about one concept, the changes affect the knowledge associated with other concepts that are represented by similar activity patterns. There have been numerous demonstrations of sensible generalization which have depended on the experimenter choosing appropriately similar patterns for diferent concepts. This paper shows how the network can be made to choose the patterns itself when shown a set of propositions that use the concepts. It chooses patterns which make explicit the underlying features that are only implicit in the propositions it is shown.},
author = {Hinton, Geoffrey E.},
booktitle = {Proceedings of the eighth annual conference of the Cognitive Science Society},
doi = {10.1109/69.917563},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/0f3d74824c0a780a0f79620813ad91ba7d7c914d.pdf:pdf},
issn = {10414347},
pages = {1--12},
title = {{Learning distributed representations of concepts}},
year = {1986}
}
@article{Hoffman1999,
abstract = {Probabilistic Latente Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrievaland filtering, natural language processing, machine learning fromtext,and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular VAlue Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. Inorder to avoid overfitting, we propose a widely applicable generalization of maximumlikelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.},
author = {Hofmann, Thomas},
doi = {10.1.1.33.1187},
isbn = {1581130961},
issn = {15206882},
journal = {Uncertainity in Artifitial Intelligence - UAI'99},
pages = {8},
title = {{Probabilistic Latent Semantic Analysis}},
year = {1999}
}
@article{Huang2012,
abstract = {Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic},
author = {Huang, Eric H and Socher, Richard and Manning, Christopher D and Ng, Andrew},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/e3e9b5257e02608794a1d0396b469869e6021d9c.pdf:pdf},
isbn = {9781937284244},
journal = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
pages = {873--882},
title = {{Improving word representations via global context and multiple word prototypes}},
year = {2012}
}
@article{Kaski1998,
abstract = {When the data vectors are high-dimensional it is computationally$\backslash$ninfeasible to use data analysis or pattern recognition algorithms which$\backslash$nrepeatedly compute similarities or distances in the original data space.$\backslash$nIt is therefore necessary to reduce the dimensionality before, for$\backslash$nexample, clustering the data. If the dimensionality is very high, like$\backslash$nin the WEBSOM method which organizes textual document collections on a$\backslash$nself-organizing map, then even the commonly used dimensionality$\backslash$nreduction methods like the principal component analysis may be too$\backslash$ncostly. It is demonstrated that the document classification accuracy$\backslash$nobtained after the dimensionality has been reduced using a random$\backslash$nmapping method will be almost as good as the original accuracy if the$\backslash$nfinal dimensionality is sufficiently large (about 100 out of 6000). In$\backslash$nfact, it can be shown that the inner product (similarity) between the$\backslash$nmapped vectors follows closely the inner product of the original vectors$\backslash$n},
author = {Kaski, S.},
doi = {10.1109/IJCNN.1998.682302},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/277d22f766007606f0723d42eb853d8fd3aff40f.pdf:pdf},
isbn = {0-7803-4859-1},
issn = {1098-7576},
journal = {1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)},
number = {1},
pages = {4--9},
title = {{Dimensionality reduction by random mapping: fast similarity computation for clustering}},
volume = {1},
year = {1998}
}
@article{Lund1995,
abstract = {We present a model of semantic memory that utilizes a high-dimensional semantic space constructed from a co-occurrence matrix. This matrix was formed by analyzing a 160 million word corpus. Word vectors were then obtained by extracting row and columns of this matrix. These vectors were subjected to multidimensional scaling. Words were found to cluster semantically, suggesting that interword distance may be interpretable as a measure of semantic similarity. In attempting to replicate with our simulation the semantic and associative priming experiment by Shelton and Martin (1992), we found that semantic similarity plays a larger role in priming than what they would suggest. Vectors were formed for three different types of related words taht may more orthogonally control for association and similarity, and interpair distances were computed for both related and unrelated prime-target pairs. A priming effect was found for pair that were only semantically related, as well as for word pairs that were both semantically and associatively related. No priming was found for word pairs which wre strictly associatively related (no semantic overlap). This finding was replicated in a single-word priming experiment using a lexical decision procedure with human subjects. The lack of associative priming is discussed in relation to prior experiments that have found robust associative priming. We conclude that our priming results are driven by semantic overlap rather than by associativity, and that prior results finding associative priming are due, at least in part, to semantic overlap within the associated word pairs.},
author = {Kevin, Ruth Ann Lund and Curt, Burgess and Atchley},
doi = {10.1016/j.jconhyd.2010.08.009},
issn = {18736009},
journal = {Cognitive Science Proceedings, LEA},
number = {JANUARY 1995},
pages = {660 -- 665},
pmid = {20926156},
title = {{Semantic and Associative Priming in High-Dimensional Semantic Space}},
year = {1995}
}
@article{LandauerDumais1997,
abstract = {How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by ex- tracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched. Prologue},
author = {Landauer, Thomas K and Dumais, Susan T.},
doi = {10.1037/0033-295X.104.2.211},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/529d7107b9a6c7862b0536236a210611fd04261a.pdf:pdf},
journal = {Psychological Review},
number = {2},
pages = {211--240},
title = {{A solution to Plato ' s problem : The Latent Semantic Analysis Theory of Acquisition , Induction , and Representation of Knowledge}},
volume = {104},
year = {1997}
}
@article{Levy2014b,
abstract = {Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.'s method of first adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.},
author = {Levy, O and Goldberg, Y and Ramat-Gan, I},
file = {:home/agustin/papers/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf:pdf},
isbn = {9781941643020},
journal = {CoNLL-2014},
pages = {171--180},
title = {{Linguistic regularities in sparse and explicit word representations}},
year = {2014}
}
@article{Levy2014a,
abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word simi-larity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
author = {Levy, Omer and Goldberg, Yoav},
file = {:home/agustin/papers/neural-word-embeddings-as-implicit-matrix-factorization.pdf:pdf},
pages = {1--9},
title = {{Neural Word Embedding as Implicit Matrix Factorization}}
}
@article{Levy2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/dc77e0371a877a5d72268e6d4f3b0136ea9a7e49.pdf:pdf},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
volume = {3},
year = {2015}
}
@article{LiMcCallum2005,
abstract = {Although there has been significant previous work on semi-supervised learning for classification, there has been relatively little in sequence modeling. This paper presents an approach that leverages recent work in manifold-learning on sequences to discover word clusters from language data, including both syntactic classes and semantic topics. From unlabeled data we form a smooth. low-dimensional feature space, where each word token is projected based on its underlying role as a function or content word. We then use this projection as additional input features to a linear-chain conditional random field trained on limited labeled training data. On standard part-of-speech tagging and Chinese word segmentation data sets we show as much as 14{\%} error reduction due to the unlabeled data, and also statistically-significant improvements over a related semi-supervised sequence tagging method due to Miller et al.},
author = {Li, Wei and McCallum, Andrew},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/3c35eb454d7cabeda8a123f7da2731b3c595b12b.pdf:pdf},
isbn = {1-57735-236-x},
journal = {Proceedings of the 20th International conference on Artificial intelligence (AAAI 2005)},
keywords = {machine learning,natural language processing},
pages = {813--818},
title = {{Semi-Supervised Sequence Modeling with Syntactic Topic Models}},
volume = {2},
year = {2005}
}
@article{LinWu2009,
abstract = {We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers. To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification. Our results show that phrase clusters offer significant improvements over word clusters. Our NER system achieves the best current result on the widely used CoNLL benchmark. Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts.},
author = {Lin, Dekang and Wu, Xiaoyun},
doi = {10.3115/1690219.1690290},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/b5e12de7d3f2a66bfcf10a510f9ac80afb3541c1.pdf:pdf},
isbn = {9781932432466},
journal = {Proceedings of the Joint Conference of the 47th Annual {\ldots}},
number = {August},
pages = {1030--1038},
title = {{Phrase clustering for discriminative learning}},
year = {2009}
}
@article{LundBurgess1996,
abstract = {A procedure that processes a corpus of text and produces numeric vectors containing information about its meanings for each word is presented. This procedure is applied to a large corpus of natural language text taken from Usenet, and the resulting vectors are examined to determine what information is contained within them. These vectors provide the coordinates in a high-dimensional space in which word relationships can be analyzed. Analyses of both vector similarity and multidimensional scaling demonstrate that there is significant semantic information carried in the vectors. A comparison of vector similarity with human reaction times in a single-word priming experiment is presented. These vectors provide the basis for a representational model of semantic memory, hyperspace analogue to language (HAL).},
author = {Lund, Kevin and Burgess, Curt},
doi = {10.3758/BF03204766},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
number = {2},
pages = {203--208},
title = {{Producing high-dimensional semantic spaces from lexical co-occurrence}},
volume = {28},
year = {1996}
}
@article{Luong2013,
abstract = {Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationship among morphologically related words being modeled. As a result, rare and complex words are often poorly estimated, and all unknown words are represented in a rather crude way using only one or a few vectors. This paper addresses this shortcoming by proposing a novel model that is capable of building representations for morphologically complex words from their morphemes. We combine recursive neural networks (RNNs), where each morpheme is a basic unit, with neural language models (NLMs) to consider contextual information in learning morphologically-aware word representations. Our learned models outperform existing word representations by a good margin on word similarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way.},
author = {Luong, Minh-Thang and Socher, Richard and Manning, Christopher D.},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/53ab89807caead278d3deb7b6a4180b277d3cb77.pdf:pdf},
isbn = {9781937284701},
journal = {CoNLL-2013},
pages = {104--113},
title = {{Better Word Representations with Recursive Neural Networks for Morphology}},
year = {2013}
}
@article{tSNE,
abstract = {We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence theway in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza- tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {Maaten, Laurens Van Der and Hinton, Geoffrey},
doi = {10.1007/s10479-011-0841-3},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/10eb7bfa7687f498268bdf74b2f60020a151bdc6.pdf:pdf},
issn = {02545330},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing Data using t-SNE}},
volume = {9},
year = {2008}
}
@article{MiikkulainenDyer1991,
author = {Miikkulainen, Risto and Dyer, Michael G},
keywords = {bibtex-import,nn},
number = {UCLA-AI-90-02},
title = {{Natural Language Processing with Modular Neural Networks and Distributed Lexicon}},
year = {1990}
}
@article{Mikolov2010,
abstract = {基于RNN模型的语言模型，详细可参考作者的博士论文。周期神经网络。但是上下文，也没用取全部的，只取到了前5个。},
author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/47a87c2cbdd928bb081974d308b3d9cf678d257e.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1045--1048},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}
@article{Mikolov2011a,
abstract = {We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one. {\&}copy; 2011 IEEE.},
author = {Mikolov, Toma and Kombrink, Stefan and Burget, Luka and Cernocky, Jan and Khudanpur, Sanjeev},
doi = {10.1109/ICASSP.2011.5947611},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/3a91c3eb9a05ef5c8b875ab112448cc3f44a1268.pdf:pdf},
isbn = {9781457705397},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Backpropagation algorithms,Computational complexity,Computational linguistics,Signal processing,Speech communication,Speech recognition},
pages = {5528--5531},
title = {{Extensions of recurrent neural network language model}},
year = {2011}
}
@article{Mikolov2007,
author = {Mikolov, Tomáš},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1975c4cd5ede97dc425eaa65d3d112468455bf1b.pdf:pdf},
journal = {Proc. of STUDENT EEICT},
number = {4},
title = {{Language Models for Automatic Speech Recognition of Czech Lectures}},
year = {2008}
}
@article{Mikolov2013b,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
file = {:home/agustin/papers/Mikolov{\_}distributed-representations-of-words-and-phrases-and-their-compositionality.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{Mikolov2013c,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:home/agustin/papers/Mikolov{\_}Efficient-Estimation-of-Word-Representations-in-Vector-Space.pdf:pdf},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
year = {2013}
}
@inproceedings{Mikolov2011b,
abstract = {We describe how to effectively train neural network based language models on large data sets. Fast convergence during training and better overall performance is observed when the training data are sorted by their relevance. We introduce hash-based implementation of a maximum entropy model, that can be trained as a part of the neural network model. This leads to significant reduction of computational complexity. We achieved around 10{\%} relative reduction of word error rate on English Broadcast News speech recognition task, against large 4-gram model trained on 400M tokens.},
author = {Mikolov, Tom{\'{a}}{\v{s}} and Deoras, Anoop and Povey, Daniel and Burget, Luk{\'{a}}{\v{s}} and {\v{C}}ernock{\'{y}}, Jan},
booktitle = {2011 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2011, Proceedings},
doi = {10.1109/ASRU.2011.6163930},
isbn = {9781467303675},
pages = {196--201},
title = {{Strategies for training large scale neural network language models}},
year = {2011}
}
@article{Mikolov2011c,
abstract = {—We present freely available open-source toolkit for training recurrent neural network based language models. It can be easily used to improve existing speech recognition and machine translation systems. Also, it can be used as a baseline for future research of advanced language modeling techniques. In the paper, we discuss optimal parameter selection and different modes of functionality. The toolkit, example scripts and basic setups are freely available at http://rnnlm.sourceforge.net/.},
author = {Mikolov, Tom{\'{a}}{\v{s}} and Kombrink, Stefan and Deoras, Anoop and Burget, Luk{\'{a}}{\v{s}} and {\v{C}}ernock{\'{y}}, Jan},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/86d62362d50fd3d26f0c049fc72d4cf40bd218b6.pdf:pdf},
journal = {Proceedings of ASRU 2011},
pages = {1--4},
title = {{RNNLM --- Recurrent Neural Network Language Modeling Toolkit}},
year = {2011}
}
@article{Mikolov2009,
abstract = {Speech recognition of inflectional and morphologically rich languages like Czech is currently quite a challenging task, because simple n-gram techniques are unable to capture important regularities in the data. Several possible solutions were proposed, namely class based models, factored models, decision trees and neural networks. This paper describes improvements obtained in recognition of spoken Czech lectures using language models based on neural networks. Relative reductions in word error rate are more than 15{\%} over baseline obtained with adapted 4-gram backoff language model using modified Kneser-Ney smoothing.},
author = {Mikolov, Tom{\'{a}}{\v{s}} and Kopeck{\'{y}}, Jiř{\'{i}} and Burget, Luk{\'{a}}{\v{s}} and Glembek, Ondřej and {\v{C}}ernock{\'{y}}, Jan Honza},
doi = {10.1109/ICASSP.2009.4960686},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/7f9e4f3e404ea350f2f9a97315ef4b1771367d8a.pdf:pdf},
isbn = {9781424423545},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Inflective languages,Language modeling,Neural networks},
pages = {4725--4728},
title = {{Neural network based language models for highly inflective languages}},
year = {2009}
}
@article{Mikolov2013a,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40{\%} of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
file = {:home/agustin/papers/Mikolov{\_}Linguistic-Regularities-in-Continuous-Space-Word-Representations.pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
title = {{Linguistic regularities in continuous space word representations}},
year = {2013}
}
@article{MnihHinton2007,
abstract = {The current study characterized the in vitro surface reactions of microroughened bioactive glasses and compared osteoblast cell responses between smooth and microrough surfaces. Three different bioactive glass compositions were used and surface microroughening was obtained using a novel chemical etching method. Porous bioactive glass specimens made of sintered microspheres were immersed in simulated body fluid (SBF) or Tris solutions for 1, 6, 24, 48, or 72 h, and the formation of reaction layers was studied by means of a scanning electron microscope/energy dispersive X-ray analysis (SEM/EDXA). Cell culture studies were performed on bioactive glass disks to examine the influence of surface microroughness on the attachment and proliferation of human osteoblast-like cells (MG-63). Cell attachment was evaluated by means of microscopic counting of in situ stained cells. Cell proliferation was analyzed with a nonradioactive cell proliferation assay combined with in situ staining and laser confocal microscopy. The microroughening of the bioactive glass surface increased the rate of the silica gel layer formation during the first hours of the immersion. The formation of calcium phosphate layer was equal between control and microroughened glass surfaces. In cell cultures on bioactive glass, the microrough surface enhanced the attachment of osteoblast-like cells but did not have an effect on the proliferation rate or morphology of the cells as compared with smooth glass surface. In conclusion, accelerated the early formation of surface reactions on three bioactive glasses and had a positive effect on initial cell attachment.},
author = {Mnih, a and Hinton, Ge},
doi = {10.1145/1273496.1273577},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/8654cde28ebb766f08a7b4cf1cb7a51d3d03b152.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th International Conference on Machine Learning (2007)},
pages = {641--648},
title = {{Three new graphical models for statistical language modelling.}},
volume = {62},
year = {2007}
}
@article{MnihHinton2009,
abstract = {Neural probabilistic language models (NPLMs) have been shown to be competi- tive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non- hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1},
author = {Mnih, Andriy and Hinton, Geoffrey E.},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/c4b23d2cc25dc9d18b4413b66fcbec806fd66224.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems},
pages = {1--8},
title = {{A Scalable Hierarchical Distributed Language Model.}},
year = {2008}
}
@article{MnihTeh2012,
abstract = {In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-of-the-art results on the Microsoft Research Sentence Completion Challenge dataset.},
archivePrefix = {arXiv},
arxivId = {1206.6426},
author = {Mnih, Andriy and Teh, Yee Whye},
eprint = {1206.6426},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/20fef32458d47cb845a3d3bfe35fd7ee567588fa.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML'12)},
pages = {1751--1758},
title = {{A Fast and Simple Algorithm for Training Neural Probabilistic Language Models}},
year = {2012}
}
@article{MorinBengio2005,
abstract = {In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.},
author = {Morin, Frederic and Bengio, Y},
doi = {10.1109/JCDL.2003.1204852},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/cd4646d73df5797122939c30929f7087838ba35c.pdf:pdf},
isbn = {0-7695-1939-3},
journal = {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
pages = {246--252},
title = {{Hierarchical probabilistic neural network language model}},
year = {2005}
}
@article{Pado2007,
abstract = {In this paper, we consider the computational modelling of human plausibility judgements for verb-relation-argument triples, a task equivalent to the computation of selectional preferences. Such models have applications both in psycholinguistics and in computational linguistics. By extending a recent model, we obtain a completely corpus-driven model for this task which achieves signiﬁcant correlations with human judgements. It rivals or exceeds deeper, resource-driven models while exhibiting higher coverage. Moreover, we show that our model can be combined with deeper models to obtain better predictions than from either model alone.},
author = {Pad{\'{o}}, Sebastian and Pad{\'{o}}, Ulrike and Erk, Katrin},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/be54384262901de303a15d9b69d484d2e2c29359.pdf:pdf},
journal = {Proceedings of EMNLP-CoNLL},
number = {June},
pages = {400--409},
title = {{Flexible, corpus-based modelling of human plausibility judgements}},
volume = {7},
year = {2007}
}
@inproceedings{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith-metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the en-tire sparse matrix or on individual context windows in a large corpus. The model pro-duces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on simi-larity tasks and named entity recognition.},
address = {Stroudsburg, PA, USA},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1162},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennington, Manning - Unknown - Glove Global vectors for word representation.pdf:pdf},
publisher = {Association for Computational Linguistics},
title = {{Glove: Global Vectors for Word Representation}},
year = {2014}
}
@article{Qu2015,
abstract = {Word embeddings -- distributed word representations that can be learned from unlabelled data -- have been shown to have high utility in many natural language processing applications. In this paper, we perform an extrinsic evaluation of five popular word embedding methods in the context of four sequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE identification. A particular focus of the paper is analysing the effects of task-based updating of word representations. We show that when using word embeddings as features, as few as several hundred training instances are sufficient to achieve competitive results, and that word embeddings lead to improvements over OOV words and out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider.},
archivePrefix = {arXiv},
arxivId = {1504.05319},
author = {Qu, Lizhen and Ferraro, Gabriela and Zhou, Liyuan and Hou, Weiwei and Schneider, Nathan and Baldwin, Timothy},
eprint = {1504.05319},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/da2a11e4579ba666b2062bd0d90a23bebc21cb3a.pdf:pdf},
journal = {CoNLL-2015},
pages = {83--93},
title = {{Big Data Small Data , In Domain Out-of Domain , Known Word Unknown Word : The Impact of Word Representations on Sequence Labelling Tasks}},
year = {2015}
}
@article{Radinsky2011,
abstract = {Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. Previous approaches to computing semantic relatedness mostly used static lan- guage resources, while essentially ignoring their temporal aspects. We believe that a considerable amount of relat- edness information can also be found in studying patterns of word usage over time. Consider, for instance, a newspa- per archive spanning many years. Two words such as “war” and “peace” might rarely co-occur in the same articles, yet their patterns of use over time might be similar. In this pa- per, we propose a new semantic relatednessmodel, Temporal Semantic Analysis (TSA), which captures this temporal in- formation. The previous state of the art method, Explicit Semantic Analysis (ESA), represented word semantics as a vector of concepts. TSA uses a more refined representation, where each concept is no longer scalar, but is instead rep- resented as time series over a corpus of temporally-ordered documents. To the best of our knowledge, this is the first attempt to incorporate temporal evidence into models of se- mantic relatedness. Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks.},
author = {Radinsky, Kira and Agichtein, Eugene and Gabrilovich, Evgeniy and Markovitch, Shaul},
doi = {10.1145/1963405.1963455},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/f90bfc24bc58a6e8434b9d92be32c3d737f21efd.pdf:pdf},
isbn = {9781450306324},
journal = {Proceedings of the 20th International World Wide Web Conference WWW'11},
keywords = {semantic analysis,semantic similarity,temporal dynamics,temporal semantics,word relatedness},
pages = {337--346},
title = {{A word at a time: computing word relatedness using temporal semantic analysis}},
year = {2011}
}
@misc{Rumelhart1986,
abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
booktitle = {Readings in Cognitive Science: A Perspective from Psychology and Artificial Intelligence},
doi = {10.1016/B978-1-4832-1446-7.50035-2},
eprint = {arXiv:1011.1669v3},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/8af0ad8d92ea33d4d88248a58d0b635b10d9f29e.pdf:pdf},
isbn = {1558600132},
issn = {1-55860-013-2},
pages = {399--421},
pmid = {25246403},
title = {{Learning Internal Representations by Error Propagation}},
year = {2013}
}
@article{Sahlgren2001,
abstract = {Vector-based semantic analysis is the practice of using co-occurrence statistics to construct vectors that represent word meanings by virtue of their direction in multi-dimensional semantic space. This paper discusses the theoretical presumptions behind this practice, and a representational scheme based on the Distributional Hypothesis is identified as the rationale for vector-based semantic analysis. A new method for calculating semantic word vectors is then described. The method uses random...},
author = {Sahlgren, Magnus},
doi = {10.1.1.20.4588},
journal = {ESSLI Workshop on Semantic Knowledge Acquistion and Categorization},
title = {{Vector-based Semantic Analysis: Representing Word Meaning Based on Random Labels}},
year = {2002}
}
@article{Socher2011b,
author = {Socher, Richard and Huang, Eric H and Pennin, Jeffrey and Manning, Christopher D and Ng, Andrew Y},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/167abf2c9eda9ce21907fcc188d2e41da37d9f0b.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {801--809},
title = {{Dynamic pooling and unfolding recursive autoencoders for paraphrase detection}},
year = {2011}
}
@article{Socher2011a,
author = {Socher, Richard and Pennington, Jeffrey and Huang, Eh},
doi = {10.1.1.224.9432},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/3bff03b7b0b0c4e8f6384dbb2a95e4338d156524.pdf:pdf},
isbn = {978-1-937284-11-4},
issn = {1937284115},
journal = {Conference on Empirical Methods in Natural Language Processing, EMNLP},
number = {i},
pages = {151--161},
title = {{Semi-supervised recursive autoencoders for predicting sentiment distributions}},
year = {2011}
}
@article{Socher2013a,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
author = {Socher, Richard and Perelygin, Alex and Wu, Jy},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/65ad0e876216ea034b7958f016456e32666bc5c6.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the {\ldots}},
pages = {1631--1642},
title = {{Recursive deep models for semantic compositionality over a sentiment treebank}},
year = {2013}
}
@article{Tian2014,
abstract = {Distributed word representations have been widely used and proven to be useful in quite a few natural language processing and text mining tasks. Most of existing word embedding models aim at generating only one embedding vector for each individual word, which, however, limits their effectiveness because huge amounts of words are polysemous (such as bank and star). To address this problem, it is necessary to build multi embedding vectors to represent different meanings of a word respectively. Some recent studies attempted to train multi-prototype word embeddings through clustering context window features of the word. However, due to a large number of parameters to train, these methods yield limited scalability and are inefficient to be trained with big data. In this paper, we introduce a much more efficient method for learning multi embedding vectors for polysemous words. In particular, we first propose to model word polysemy from a probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model. Under this framework, we design an Expectation-Maximization algorithm to learn the word's multi embedding vectors. With much less parameters to train, our model can achieve comparable or even better results on word-similarity tasks compared with conventional methods.},
author = {Tian, Fei and Dai, Hanjun and Bian, Jiang and Gao, Bin and Zhang, Rui and Chen, Enhong and Liu, Tie-Yan},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1a99d1e2e92ee1abb810bee2aa72dda9a1b413e4.pdf:pdf},
journal = {Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014)},
number = {5},
pages = {151--160},
title = {{A probabilistic model for learning multi-prototype word embeddings}},
year = {2014}
}
@article{Turian2010,
abstract = {If we take an existing supervised {\{}NLP{\}} system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and {\{}HLBL{\}} (Mnih {\&} Hinton, 2009) embeddings of words on both {\{}NER{\}} and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing {\{}NLP{\}} systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/},
author = {Turian, Joseph and Ratinov, Lev and Bengio, Yoshua and Turian, Joseph},
doi = {10.1.1.301.5840},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/4c15b129a8da55127e4e2fe47f54799d0a313367.pdf:pdf},
isbn = {9781617388088},
journal = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
number = {July},
pages = {384--394},
title = {{Word Representations: A Simple and General Method for Semi-supervised Learning}},
year = {2010}
}
@article{Turney2001,
abstract = {This paper presents a simple unsupervised learning algorithm for rec- ognizing synonyms, based on statistical data acquired by querying a Web search engine. The algorithm, called PMI-IR, uses Pointwise Mutual Information (PMI) and Information Retrieval (IR) to measure the similarity of pairs of words. PMI-IR is empirically evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 synonym test ques- tions from a collection of tests for students of English as a Second Language (ESL). On both tests, the algorithm obtains a score of 74{\%}. PMI-IR is con- trasted with Latent Semantic Analysis (LSA), which achieves a score of 64{\%} on the same 80 TOEFL questions. The paper discusses potential applications of the new unsupervised learning algorithm and some implications of the results for LSA and LSI (Latent Semantic Indexing).},
archivePrefix = {arXiv},
arxivId = {cs/0212033},
author = {Turney, Peter D},
doi = {10.1007/3-540-44795-4_42},
eprint = {0212033},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/59d97d6d76eff9238bb0dcadd416ec9523d204af.pdf:pdf},
isbn = {3-540-42536-5},
issn = {16113349},
journal = {Proceedings of the 12th European Conference on Machine Learning (ECML-2001), Freiburg, Germany},
keywords = {Distributional semantics},
pages = {491--502},
primaryClass = {cs},
title = {{Mining the Web for synonyms: PMI-IR versus LSA on TOEFL}},
year = {2001}
}
@article{VayrynenHonkela2004,
author = {V{\"{a}}yrynen, Jaakko J and Honkela, Timo},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/82af8ff2fbdff52ae3a11943c062ba4adf40d988.pdf:pdf},
title = {{Word Category Maps based on Emergent Features Created by ICA}},
}
@article{Socher2013b,
abstract = {We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task.},
author = {Zou, Will Y and Socher, Richard and Cer, Daniel and Manning, Christopher D},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/0d3233d858660aff451a6c2561a05378ed09725a.pdf:pdf},
isbn = {9781937284978},
journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)},
number = {October},
pages = {1393--1398},
title = {{Bilingual Word Embeddings for Phrase-Based Machine Translation}},
year = {2013}
}
@article{Eisele2010,
abstract = {This paper describes the acquisition, preparation and properties of a corpus extracted from the official documents of the United Nations (UN). This corpus is available in all 6 official languages of the UN, consisting of around 300 millionwords per language. We describe the methods we used for crawling, document formatting, and sentence alignment. This corpus also includes a common test set for machine translation. We present the results of a French-Chinese machine translation experiment performed on this corpus.},
author = {Eisele, Andreas and Chen, Yu},
file = {:home/agustin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/bcd6de970fb4b91262f3ced4abc05694a70aa751.:},
isbn = {2-9517408-6-7},
journal = {Lrec},
pages = {2868--2872},
title = {{MultiUN: A Multilingual Corpus from United Nation Documents.}},
year = {2010}
}
@article{RehurekLrec,
title = {{Software Framework for Topic Modelling with Large Corpora}},
author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
booktitle = {{Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks}},
pages = {45--50},
publisher = {ELRA},
address = {Valletta, Malta},
year = {2010}
}
@article{Leff2001,
author = {Leff, Avraham and Rayfield, James T.},
title = {Web-Application Development Using the Model/View/Controller Design Pattern},
booktitle = {Proceedings of the 5th IEEE International Conference on Enterprise Distributed Object Computing},
isbn = {0-7695-1345-X},
pages = {118--},
publisher = {IEEE Computer Society},
address = {Washington, DC, USA},
year = {2001}
}
@article{Fielding2000,
author = {Fielding, Roy},
title = {Architectural Styles and the Design of Network-based Software Architectures},
year = {2000}
}
